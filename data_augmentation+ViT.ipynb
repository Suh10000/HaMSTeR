{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce14b5e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1612fd7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12806282963372238751\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6940786688\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8229374723827501161\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c546c5e6",
   "metadata": {},
   "source": [
    "## Data Augmentation Code\n",
    "https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#gaussianblur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43ffde1",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03cd31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "print('torch version:', torch.__version__)\n",
    "\n",
    "word_types = ['1', '2', '3', '4', '가구', '간섭', '개폐', '거실', '고정', '공', '과다', '기타', '누락', '단차', '대피공간', '도배', '도장', '드레스룸', '들뜸', '마감', '마감판', '면불량', '문', '미시공', '미흡', '바닥', '발코니', '벽', '부', '불량', '붙박이장', '석고', '세부위치', '세탁실', '수직수평', '스크레치', '실링', '실외기실', '안', '알', '오염', '욕실', '유격', '이격', '이음불량', '작동', '잠금', '전등', '주방', '줄눈', '찍힘', '창호', '천장', '침', '코킹', '콘센트', '타일', '태움', '틀', '틈새', '파손', '파우더', '팬트리', '현관', '확인']\n",
    "ROOT_DIR = r'C:\\Users\\01_data\\aug_original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3863b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fa0001f",
   "metadata": {},
   "source": [
    "### Tick Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb592608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original images: 65\n",
      "Example of file name: 1.jpg\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = r'C:\\Users\\01_data\\aug_original'\n",
    "original_list = os.listdir(os.path.join(ROOT_DIR,'words/'))\n",
    "\n",
    "print('Number of original images:', len(original_list))\n",
    "print('Example of file name:', original_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7efdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# white = Image.new('RGB', (1000, 1000), (255,255,255))\n",
    "# white.save(os.path.join(ROOT_DIR,'white.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37a94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circle augmentation\n",
    "for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'tick'))):\n",
    "    \n",
    "    # translation, scaling by Affine transfrom\n",
    "    def AffineTransform(img_dir, new_dir):\n",
    "        orig_img = Image.open(img_dir)\n",
    "        affine_transformer = T.RandomAffine(degrees=(0, 0), translate=(0.2, 0.2), scale=(0.5, 1.5))\n",
    "        affine_imgs = [affine_transformer(orig_img) for _ in range(7)]\n",
    "        for i, affine_img in enumerate(affine_imgs):\n",
    "            affine_img.save(f'{new_dir}_a_{i}.png')\n",
    "    \n",
    "    full_dir = os.path.join(ROOT_DIR, 'tick', circle_dir)\n",
    "    new_dir = os.path.join(ROOT_DIR, 'aug_tick', circle_dir[:-4])\n",
    "    \n",
    "    AffineTransform(full_dir, new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99538231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.png is done\n",
      "2.png is done\n",
      "3.png is done\n",
      "4.png is done\n",
      "가구.png is done\n",
      "간섭.png is done\n",
      "개폐.png is done\n",
      "거실.png is done\n",
      "고정.png is done\n",
      "공.png is done\n",
      "과다.png is done\n",
      "기타.png is done\n",
      "누락.png is done\n",
      "단차.png is done\n",
      "대피공간.png is done\n",
      "도배.png is done\n",
      "도장.png is done\n",
      "드레스룸.png is done\n",
      "들뜸.png is done\n",
      "마감.png is done\n",
      "마감판.png is done\n",
      "면불량.png is done\n",
      "문.png is done\n",
      "미시공.png is done\n",
      "미흡.png is done\n",
      "바닥.png is done\n",
      "발코니.png is done\n",
      "벽.png is done\n",
      "부.png is done\n",
      "불량.png is done\n",
      "붙박이장.png is done\n",
      "석고.png is done\n",
      "세부위치.png is done\n",
      "세탁실.png is done\n",
      "수직수평.png is done\n",
      "스크레치.png is done\n",
      "실링.png is done\n",
      "실외기실.png is done\n",
      "안.png is done\n",
      "알.png is done\n",
      "오염.png is done\n",
      "욕실.png is done\n",
      "유격.png is done\n",
      "이격.png is done\n",
      "이음불량.png is done\n",
      "작동.png is done\n",
      "잠금.png is done\n",
      "전등.png is done\n",
      "주방.png is done\n",
      "줄눈.png is done\n",
      "찍힘.png is done\n",
      "창호.png is done\n",
      "천장.png is done\n",
      "침.png is done\n",
      "코킹.png is done\n",
      "콘센트.png is done\n",
      "타일.png is done\n",
      "태움.png is done\n",
      "틀.png is done\n",
      "틈새.png is done\n",
      "파손.png is done\n",
      "파우더.png is done\n",
      "팬트리.png is done\n",
      "현관.png is done\n",
      "확인.png is done\n",
      "All finished\n"
     ]
    }
   ],
   "source": [
    "# Read image\n",
    "def add_circle(word_dir):\n",
    "    word = Image.open(os.path.join(ROOT_DIR, 'words', word_dir))\n",
    "    \n",
    "    for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'aug_tick'))):\n",
    "        # Resize the circle image to fit circle\n",
    "        word2 = word.copy()\n",
    "        w, h = word2.size\n",
    "        \n",
    "        circle = Image.open(os.path.join(ROOT_DIR, 'aug_tick', circle_dir))\n",
    "        circle2 = circle.resize((w, h))\n",
    "        w2, h2 = circle2.size\n",
    "\n",
    "#         if w >= h:\n",
    "#             circle3 = circle2.crop((0, h2/2-h/2, w, h2/2+h/2))\n",
    "#         else:\n",
    "#             circle3 = circle2.crop((w2/2-w/2, 0, w2/2+w/2, h))\n",
    "#         circle3 = circle3.resize((w, h))\n",
    "\n",
    "        # paste circle to word image\n",
    "#         im = Image.alpha_composite(word2.convert('RGBA'), circle3.convert('RGBA'))\n",
    "        im = Image.alpha_composite(word2.convert('RGBA'), circle2.convert('RGBA'))\n",
    "        \n",
    "        # Make background white\n",
    "        white = Image.open(os.path.join(ROOT_DIR, 'white.jpg'))\n",
    "        white2 = white.resize(im.size)\n",
    "        \n",
    "        im2 = Image.alpha_composite(white2.convert('RGBA'), im)\n",
    "        im2.convert('RGB').save(f'{ROOT_DIR}/aug_tick_word/{word_dir[:-4]}_{i}.jpg')\n",
    "        \n",
    "        \n",
    "for word_dir in os.listdir(os.path.join(ROOT_DIR, 'words')):\n",
    "    add_circle(word_dir)\n",
    "    print(word_dir, 'is done')\n",
    "print('All finished')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9d39d94",
   "metadata": {},
   "source": [
    "### Random Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bddcd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianBlur(img_dir, new_dir):\n",
    "    orig_img = Image.open(img_dir)\n",
    "    blurrer = T.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 10))   \n",
    "    blurred_imgs = [blurrer(orig_img) for _ in range(3)]\n",
    "    for i, blurred_img in enumerate(blurred_imgs):\n",
    "        blurred_img.save(new_dir+'_g_'+str(i)+'.jpg')\n",
    "\n",
    "    \n",
    "def PerspectiveTransform(img_dir, new_dir):\n",
    "    orig_img = Image.open(img_dir)\n",
    "    perspective_transformer = T.RandomPerspective(distortion_scale=0.5, p=1.0, fill=(255,255,255))\n",
    "    perspective_imgs = [perspective_transformer(orig_img) for _ in range(5)]\n",
    "    for i, perspective_img in enumerate(perspective_imgs):\n",
    "        perspective_img.save(new_dir+'_p_'+str(i)+'.jpg')\n",
    "\n",
    "    \n",
    "# def AffineTransform(img_dir, new_dir):\n",
    "#     orig_img = Image.open(img_dir)\n",
    "#     affine_transformer = T.RandomAffine(degrees=(-20, 20), translate=(0.2, 0.2), scale=(0.5, 2))\n",
    "#     affine_imgs = [affine_transformer(orig_img) for _ in range(7)]\n",
    "#     for i, affine_img in enumerate(affine_imgs):\n",
    "#         affine_img.save(new_dir+'_a_'+str(i)+'.jpg')\n",
    "        \n",
    "        \n",
    "# def Brightness(img_dir, new_dir):\n",
    "#     orig_img = Image.open(img_dir)\n",
    "#     for i in range(5):\n",
    "#         brightness_adjuster = T.functional.adjust_brightness(brightness_factor=np.random.rand(1))\n",
    "#         brightness_img = brightness_adjuster(orig_img)\n",
    "#         brightness_img.save(new_dir+'_b_'+str(i)+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ceb400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4550/4550 [07:47<00:00,  9.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_tick_word/')):\n",
    "    img_dir = ROOT_DIR+'/aug_tick_word/'+img_file\n",
    "    \n",
    "    new_dir_g = ROOT_DIR+'/aug_tick_word2/GaussianBlurred/'+img_file[:-4]\n",
    "    GaussianBlur(img_dir, new_dir_g) \n",
    "    \n",
    "#     new_dir_p = ROOT_DIR+'aug_result/PerspectiveTransformed/'+img_file[:-4]\n",
    "#     PerspectiveTransform(img_dir, new_dir_p)\n",
    "    \n",
    "#     new_dir_a = ROOT_DIR+'aug_result/AffineTransformed/'+img_file[:-4]\n",
    "#     AffineTransform(img_dir, new_dir_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09571d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13650/13650 [24:12<00:00,  9.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_tick_word2/GaussianBlurred/')):\n",
    "    img_dir = ROOT_DIR+'/aug_tick_word2/GaussianBlurred/'+img_file\n",
    "    \n",
    "    new_dir_p = ROOT_DIR+'/aug_tick_word2/PerspectiveTransformed/'+img_file[:-4]\n",
    "    PerspectiveTransform(img_dir, new_dir_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1366b068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gaussian blurred file: 13650\n",
      "Number of perspective transformed file: 68250\n"
     ]
    }
   ],
   "source": [
    "gaussian_dir = ROOT_DIR+'/aug_tick_word2/GaussianBlurred/'\n",
    "perspective_dir = ROOT_DIR+'/aug_tick_word2/PerspectiveTransformed/'\n",
    "# affine_dir = ROOT_DIR+'aug_result/AffineTransformed/'\n",
    "# brightness_dir = ROOT_DIR+'aug_result/Brightness/'\n",
    "\n",
    "print('Number of gaussian blurred file:', len(os.listdir(gaussian_dir)))\n",
    "print('Number of perspective transformed file:', len(os.listdir(perspective_dir)))\n",
    "# print('Number of affine transformed file:', len(os.listdir(affine_dir)))\n",
    "# print('total:', 650+len(os.listdir(gaussian_dir))+len(os.listdir(perspective_dir))+len(os.listdir(affine_dir)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c5c2921",
   "metadata": {},
   "source": [
    "### Star Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d50fe984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circle augmentation\n",
    "for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'star'))):\n",
    "    \n",
    "    # translation, scaling by Affine transfrom\n",
    "    def AffineTransform(img_dir, new_dir):\n",
    "        orig_img = Image.open(img_dir)\n",
    "        affine_transformer = T.RandomAffine(degrees=(0, 0), translate=(0.2, 0.2), scale=(0.5, 1.5))\n",
    "        affine_imgs = [affine_transformer(orig_img) for _ in range(7)]\n",
    "        for i, affine_img in enumerate(affine_imgs):\n",
    "            affine_img.save(f'{new_dir}_star_a_{i}.png')\n",
    "    \n",
    "    full_dir = os.path.join(ROOT_DIR, 'star', circle_dir)\n",
    "    new_dir = os.path.join(ROOT_DIR, 'aug_star', circle_dir[:-4])\n",
    "    \n",
    "    AffineTransform(full_dir, new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da39ed66",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.png is done\n",
      "2.png is done\n",
      "3.png is done\n",
      "4.png is done\n",
      "가구.png is done\n",
      "간섭.png is done\n",
      "개폐.png is done\n",
      "거실.png is done\n",
      "고정.png is done\n",
      "공.png is done\n",
      "과다.png is done\n",
      "기타.png is done\n",
      "누락.png is done\n",
      "단차.png is done\n",
      "대피공간.png is done\n",
      "도배.png is done\n",
      "도장.png is done\n",
      "드레스룸.png is done\n",
      "들뜸.png is done\n",
      "마감.png is done\n",
      "마감판.png is done\n",
      "면불량.png is done\n",
      "문.png is done\n",
      "미시공.png is done\n",
      "미흡.png is done\n",
      "바닥.png is done\n",
      "발코니.png is done\n",
      "벽.png is done\n",
      "부.png is done\n",
      "불량.png is done\n",
      "붙박이장.png is done\n",
      "석고.png is done\n",
      "세부위치.png is done\n",
      "세탁실.png is done\n",
      "수직수평.png is done\n",
      "스크레치.png is done\n",
      "실링.png is done\n",
      "실외기실.png is done\n",
      "안.png is done\n",
      "알.png is done\n",
      "오염.png is done\n",
      "욕실.png is done\n",
      "유격.png is done\n",
      "이격.png is done\n",
      "이음불량.png is done\n",
      "작동.png is done\n",
      "잠금.png is done\n",
      "전등.png is done\n",
      "주방.png is done\n",
      "줄눈.png is done\n",
      "찍힘.png is done\n",
      "창호.png is done\n",
      "천장.png is done\n",
      "침.png is done\n",
      "코킹.png is done\n",
      "콘센트.png is done\n",
      "타일.png is done\n",
      "태움.png is done\n",
      "틀.png is done\n",
      "틈새.png is done\n",
      "파손.png is done\n",
      "파우더.png is done\n",
      "팬트리.png is done\n",
      "현관.png is done\n",
      "확인.png is done\n",
      "All finished\n"
     ]
    }
   ],
   "source": [
    "# Read image\n",
    "def add_circle(word_dir):\n",
    "    word = Image.open(os.path.join(ROOT_DIR, 'words', word_dir))\n",
    "    \n",
    "    for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'aug_star'))):\n",
    "        # Resize the circle image to fit circle\n",
    "        word2 = word.copy()\n",
    "        w, h = word2.size\n",
    "        \n",
    "        circle = Image.open(os.path.join(ROOT_DIR, 'aug_star', circle_dir))\n",
    "        circle2 = circle.resize((w, h))\n",
    "        w2, h2 = circle2.size\n",
    "\n",
    "        im = Image.alpha_composite(word2.convert('RGBA'), circle2.convert('RGBA'))\n",
    "        \n",
    "        # Make background white\n",
    "        white = Image.open(os.path.join(ROOT_DIR, 'white.jpg'))\n",
    "        white2 = white.resize(im.size)\n",
    "        \n",
    "        im2 = Image.alpha_composite(white2.convert('RGBA'), im)\n",
    "        im2.convert('RGB').save(f'{ROOT_DIR}/aug_star_word/{word_dir[:-4]}_star_{i}.jpg')\n",
    "        \n",
    "        \n",
    "for word_dir in os.listdir(os.path.join(ROOT_DIR, 'words')):\n",
    "    add_circle(word_dir)\n",
    "    print(word_dir, 'is done')\n",
    "print('All finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce464cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13650/13650 [19:08<00:00, 11.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_star_word/')):\n",
    "    img_dir = ROOT_DIR+'/aug_star_word/'+img_file\n",
    "    \n",
    "    new_dir_g = ROOT_DIR+'/aug_star_word2/GaussianBlurred/'+img_file[:-4]\n",
    "    GaussianBlur(img_dir, new_dir_g) \n",
    "    \n",
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_star_word2/GaussianBlurred/')):\n",
    "    img_dir = ROOT_DIR+'/aug_star_word2/GaussianBlurred/'+img_file\n",
    "    \n",
    "    new_dir_p = ROOT_DIR+'/aug_star_word2/PerspectiveTransformed/'+img_file[:-4]\n",
    "    PerspectiveTransform(img_dir, new_dir_p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9bd217b",
   "metadata": {},
   "source": [
    "### Circle Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37a51538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circle augmentation\n",
    "for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'circle'))):\n",
    "    \n",
    "    # translation, scaling by Affine transfrom\n",
    "    def AffineTransform(img_dir, new_dir):\n",
    "        orig_img = Image.open(img_dir)\n",
    "        affine_transformer = T.RandomAffine(degrees=(0, 0), translate=(0.2, 0.2), scale=(0.5, 1.5))\n",
    "        affine_imgs = [affine_transformer(orig_img) for _ in range(7)]\n",
    "        for i, affine_img in enumerate(affine_imgs):\n",
    "            affine_img.save(f'{new_dir}_cir_a_{i}.png')\n",
    "    \n",
    "    full_dir = os.path.join(ROOT_DIR, 'circle', circle_dir)\n",
    "    new_dir = os.path.join(ROOT_DIR, 'aug_circle', circle_dir[:-4])\n",
    "    \n",
    "    AffineTransform(full_dir, new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4381ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.png is done\n",
      "2.png is done\n",
      "3.png is done\n",
      "4.png is done\n",
      "가구.png is done\n",
      "간섭.png is done\n",
      "개폐.png is done\n",
      "거실.png is done\n",
      "고정.png is done\n",
      "공.png is done\n",
      "과다.png is done\n",
      "기타.png is done\n",
      "누락.png is done\n",
      "단차.png is done\n",
      "대피공간.png is done\n",
      "도배.png is done\n",
      "도장.png is done\n",
      "드레스룸.png is done\n",
      "들뜸.png is done\n",
      "마감.png is done\n",
      "마감판.png is done\n",
      "면불량.png is done\n",
      "문.png is done\n",
      "미시공.png is done\n",
      "미흡.png is done\n",
      "바닥.png is done\n",
      "발코니.png is done\n",
      "벽.png is done\n",
      "부.png is done\n",
      "불량.png is done\n",
      "붙박이장.png is done\n",
      "석고.png is done\n",
      "세부위치.png is done\n",
      "세탁실.png is done\n",
      "수직수평.png is done\n",
      "스크레치.png is done\n",
      "실링.png is done\n",
      "실외기실.png is done\n",
      "안.png is done\n",
      "알.png is done\n",
      "오염.png is done\n",
      "욕실.png is done\n",
      "유격.png is done\n",
      "이격.png is done\n",
      "이음불량.png is done\n",
      "작동.png is done\n",
      "잠금.png is done\n",
      "전등.png is done\n",
      "주방.png is done\n",
      "줄눈.png is done\n",
      "찍힘.png is done\n",
      "창호.png is done\n",
      "천장.png is done\n",
      "침.png is done\n",
      "코킹.png is done\n",
      "콘센트.png is done\n",
      "타일.png is done\n",
      "태움.png is done\n",
      "틀.png is done\n",
      "틈새.png is done\n",
      "파손.png is done\n",
      "파우더.png is done\n",
      "팬트리.png is done\n",
      "현관.png is done\n",
      "확인.png is done\n",
      "All finished\n"
     ]
    }
   ],
   "source": [
    "# Read image\n",
    "def add_circle(word_dir):\n",
    "    word = Image.open(os.path.join(ROOT_DIR, 'words', word_dir))\n",
    "    \n",
    "    for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'aug_circle'))):\n",
    "        # Resize the circle image to fit circle\n",
    "        word2 = word.copy()\n",
    "        w, h = word2.size\n",
    "        \n",
    "        circle = Image.open(os.path.join(ROOT_DIR, 'aug_circle', circle_dir))\n",
    "        circle2 = circle.resize((w, h))\n",
    "        w2, h2 = circle2.size\n",
    "\n",
    "        im = Image.alpha_composite(word2.convert('RGBA'), circle2.convert('RGBA'))\n",
    "        \n",
    "        # Make background white\n",
    "        white = Image.open(os.path.join(ROOT_DIR, 'white.jpg'))\n",
    "        white2 = white.resize(im.size)\n",
    "        \n",
    "        im2 = Image.alpha_composite(white2.convert('RGBA'), im)\n",
    "        im2.convert('RGB').save(f'{ROOT_DIR}/aug_circle_word/{word_dir[:-4]}_cir_{i}.jpg')\n",
    "        \n",
    "        \n",
    "for word_dir in os.listdir(os.path.join(ROOT_DIR, 'words')):\n",
    "    add_circle(word_dir)\n",
    "    print(word_dir, 'is done')\n",
    "print('All finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "038cff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13650/13650 [17:59<00:00, 12.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_circle_word/')):\n",
    "    img_dir = ROOT_DIR+'/aug_circle_word/'+img_file\n",
    "    \n",
    "    new_dir_g = ROOT_DIR+'/aug_circle_word2/GaussianBlurred/'+img_file[:-4]\n",
    "    GaussianBlur(img_dir, new_dir_g) \n",
    "    \n",
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_circle_word2/GaussianBlurred/')):\n",
    "    img_dir = ROOT_DIR+'/aug_circle_word2/GaussianBlurred/'+img_file\n",
    "    \n",
    "    new_dir_p = ROOT_DIR+'/aug_circle_word2/PerspectiveTransformed/'+img_file[:-4]\n",
    "    PerspectiveTransform(img_dir, new_dir_p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bfac2fb",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53c675ca",
   "metadata": {},
   "source": [
    "Create folders for each semantic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5144dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "word_types = ['1', '2', '3', '4', '가구', '간섭', '개폐', '거실', '고정', '공', '과다', '기타', '누락', '단차', '대피공간', '도배', '도장', '드레스룸', '들뜸', '마감', '마감판', '면불량', '문', '미시공', '미흡', '바닥', '발코니', '벽', '부', '불량', '붙박이장', '석고', '세부위치', '세탁실', '수직수평', '스크레치', '실링', '실외기실', '안', '알', '오염', '욕실', '유격', '이격', '이음불량', '작동', '잠금', '전등', '주방', '줄눈', '찍힘', '창호', '천장', '침', '코킹', '콘센트', '타일', '태움', '틀', '틈새', '파손', '파우더', '팬트리', '현관', '확인']\n",
    "\n",
    "for word in word_types:\n",
    "    dd_directory = os.path.join('C:/Users/01_data/general_train', word)\n",
    "    os.makedirs(dd_directory)\n",
    "    \n",
    "for word in word_types:\n",
    "    dd_directory = os.path.join('C:/Users/01_data/general_val', word)\n",
    "    os.makedirs(dd_directory)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed7ecb35",
   "metadata": {},
   "source": [
    "1) Real world images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d65477cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SaveCropped(txt_dir, start_index):\n",
    "#     with open(txt_dir, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     DIR_CROPPED = '/home/deep-text-recognition-benchmark/new_data'\n",
    "#     DIR_NEW_TRAIN = '/home/word_train/'\n",
    "#     DIR_NEW_VAL = '/home/word_val/'\n",
    "#     DIR_NEW_TEST = '/home/word_test/'\n",
    "    \n",
    "#     lines222 = lines[start_index:]\n",
    "\n",
    "#     for i, line in enumerate(lines222):\n",
    "#         data = line.strip().split('\t')\n",
    "#         data_dir_full = DIR_CROPPED + data[0][10:]\n",
    "        \n",
    "#         index = data[0].rfind('/')\n",
    "#         data_name = data[0][index:]\n",
    "#         data_type = data[1]\n",
    "        \n",
    "#         if i < len(lines222)*0.8:\n",
    "#             new_dir_full = DIR_NEW_TRAIN + data_type + data_name\n",
    "#         elif i < len(lines222)*0.9:\n",
    "#             new_dir_full = DIR_NEW_VAL + data_type + data_name\n",
    "#         else:\n",
    "#             new_dir_full = DIR_NEW_TEST + data_type + data_name\n",
    "\n",
    "#         img = Image.open(data_dir_full)\n",
    "#         img.save(new_dir_full)\n",
    "        \n",
    "# SaveCropped('/home/deep-text-recognition-benchmark/new_data/gt_validation.txt', 5309)\n",
    "# SaveCropped('/home/deep-text-recognition-benchmark/new_data/gt_train.txt', 25863)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a7df52f",
   "metadata": {},
   "source": [
    "2) 만든 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "886e106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Big/Documents/01_Project/202101_XAI/01_data/aug_original/aug_tick_word\\1_0.jpg\n"
     ]
    }
   ],
   "source": [
    "hello = {}\n",
    "for word_type in word_types:\n",
    "    hello[word_type] = []\n",
    "\n",
    "def LetmeClassify(root_dir):\n",
    "    for file_name in glob.glob(root_dir+'/*.jpg'):\n",
    "        short_name = file_name[file_name.rfind('\\\\')+1:]\n",
    "        word_type = short_name[:short_name.find('_')]\n",
    "        hello[word_type].append(file_name)\n",
    "        \n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_tick_word')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_tick_word2/GaussianBlurred')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_tick_word2/PerspectiveTransformed')\n",
    "\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_star_word')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_star_word2/GaussianBlurred')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_star_word2/PerspectiveTransformed')\n",
    "\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_circle_word')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_circle_word2/GaussianBlurred')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_circle_word2/PerspectiveTransformed')\n",
    "\n",
    "print(hello['1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd59a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3990\n"
     ]
    }
   ],
   "source": [
    "print(len(hello['1']))\n",
    "# print(len(hello['2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af602979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: 1\n",
      "Finished: 2\n",
      "Finished: 3\n",
      "Finished: 4\n",
      "Finished: 가구\n",
      "Finished: 간섭\n",
      "Finished: 개폐\n",
      "Finished: 거실\n",
      "Finished: 고정\n",
      "Finished: 공\n",
      "Finished: 과다\n",
      "Finished: 기타\n",
      "Finished: 누락\n",
      "Finished: 단차\n",
      "Finished: 대피공간\n",
      "Finished: 도배\n",
      "Finished: 도장\n",
      "Finished: 드레스룸\n",
      "Finished: 들뜸\n",
      "Finished: 마감\n",
      "Finished: 마감판\n",
      "Finished: 면불량\n",
      "Finished: 문\n",
      "Finished: 미시공\n",
      "Finished: 미흡\n",
      "Finished: 바닥\n",
      "Finished: 발코니\n",
      "Finished: 벽\n",
      "Finished: 부\n",
      "Finished: 불량\n",
      "Finished: 붙박이장\n",
      "Finished: 석고\n",
      "Finished: 세부위치\n",
      "Finished: 세탁실\n",
      "Finished: 수직수평\n",
      "Finished: 스크레치\n",
      "Finished: 실링\n",
      "Finished: 실외기실\n",
      "Finished: 안\n",
      "Finished: 알\n",
      "Finished: 오염\n",
      "Finished: 욕실\n",
      "Finished: 유격\n",
      "Finished: 이격\n",
      "Finished: 이음불량\n",
      "Finished: 작동\n",
      "Finished: 잠금\n",
      "Finished: 전등\n",
      "Finished: 주방\n",
      "Finished: 줄눈\n",
      "Finished: 찍힘\n",
      "Finished: 창호\n",
      "Finished: 천장\n",
      "Finished: 침\n",
      "Finished: 코킹\n",
      "Finished: 콘센트\n",
      "Finished: 타일\n",
      "Finished: 태움\n",
      "Finished: 틀\n",
      "Finished: 틈새\n",
      "Finished: 파손\n",
      "Finished: 파우더\n",
      "Finished: 팬트리\n",
      "Finished: 현관\n",
      "Finished: 확인\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "DIR_NEW_TRAIN = r'C:\\Users\\01_data\\general_train'\n",
    "DIR_NEW_VAL = r'C:\\Users\\01_data\\general_val'\n",
    "DIR_NEW_TEST = r'C:\\Users\\01_data\\general_test'\n",
    "\n",
    "import random\n",
    "\n",
    "for word_type in word_types:\n",
    "    file_name_list = random.sample(list(hello[word_type]), len(hello['1']))\n",
    "    \n",
    "    for i, file_name in enumerate(file_name_list):\n",
    "        short_name = file_name[file_name.rfind('\\\\')+1:]\n",
    "        img = Image.open(file_name)\n",
    "        \n",
    "        if i < 0.8*len(hello['1']):\n",
    "            img.save(os.path.join(DIR_NEW_TRAIN, word_type, short_name))\n",
    "        else:\n",
    "            img.save(os.path.join(DIR_NEW_VAL, word_type, short_name))\n",
    "            \n",
    "    print(f'Finished: {word_type}')\n",
    "print('done')        \n",
    "#         if i < 0.6*2450:\n",
    "#             img.save(os.path.join(DIR_NEW_TRAIN, word_type, short_name))\n",
    "#         elif i < 0.8*2450:\n",
    "#             img.save(os.path.join(DIR_NEW_VAL, word_type, short_name))\n",
    "#         else:\n",
    "#             img.save(os.path.join(DIR_NEW_TEST, word_type, short_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78d579e2",
   "metadata": {},
   "source": [
    "Circle - save at the corresponding folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fe908f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello = {}\n",
    "# for word_type in word_types:\n",
    "#     hello[word_type] = []\n",
    "\n",
    "# LetmeClassify('C:/Users/Big/Documents/01_Project/202101_XAI/01_data/aug_original/aug_circle_word')\n",
    "# LetmeClassify('C:/Users/Big/Documents/01_Project/202101_XAI/01_data/aug_original/aug_circle_word2/GaussianBlurred')\n",
    "# LetmeClassify('C:/Users/Big/Documents/01_Project/202101_XAI/01_data/aug_original/aug_circle_word2/PerspectiveTransformed')\n",
    "    \n",
    "# for word_type in word_types:\n",
    "#     file_name_list = random.sample(list(hello[word_type]), 1330)\n",
    "    \n",
    "#     for i, file_name in enumerate(file_name_list):\n",
    "#         short_name = file_name[file_name.rfind('\\\\')+1:]\n",
    "#         img = Image.open(file_name)\n",
    "        \n",
    "#         if i < 0.6*1330:\n",
    "#             img.save(os.path.join(DIR_NEW_TRAIN, word_type, short_name))\n",
    "#         elif i < 0.8*1330:\n",
    "#             img.save(os.path.join(DIR_NEW_VAL, word_type, short_name))\n",
    "#         else:\n",
    "#             img.save(os.path.join(DIR_NEW_TEST, word_type, short_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bad28277",
   "metadata": {},
   "source": [
    "### Let's make \"full tag img\" for YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90e8cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "word_types = ['1', '2', '3', '4', '가구', '간섭', '개폐', '거실', '고정', '공', '과다', '기타', '누락', '단차', '대피공간', '도배', '도장', '드레스룸', '들뜸', '마감', '마감판', '면불량', '문', '미시공', '미흡', '바닥', '발코니', '벽', '부', '불량', '붙박이장', '석고', '세부위치', '세탁실', '수직수평', '스크레치', '실링', '실외기실', '안', '알', '오염', '욕실', '유격', '이격', '이음불량', '작동', '잠금', '전등', '주방', '줄눈', '찍힘', '창호', '천장', '침', '코킹', '콘센트', '타일', '태움', '틀', '틈새', '파손', '파우더', '팬트리', '현관', '확인']\n",
    "ROOT_DIR = r'C:\\Users\\01_data\\aug_original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "787a4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# white = Image.new('RGB', (10000, 10000), (255,255,255))\n",
    "# white.save(os.path.join(ROOT_DIR,'white2.jpg'))\n",
    "\n",
    "# # Make background white\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# import glob\n",
    "\n",
    "# white = Image.open(os.path.join(r'C:\\Users\\Big\\Documents\\01_Project\\202101_XAI\\01_data\\aug_original', 'white.jpg'))\n",
    "\n",
    "# for file_dir in glob.glob(r'C:\\Users\\Big\\Documents\\01_Project\\202101_XAI\\01_data\\aug_original\\words2'+'/*.png'):\n",
    "#     word_dir = file_dir[file_dir.rfind('\\\\')+1:]\n",
    "#     im = Image.open(file_dir)\n",
    "#     white2 = white.resize(im.size)\n",
    "    \n",
    "#     im2 = Image.alpha_composite(white2.convert('RGBA'), im.convert('RGBA'))\n",
    "#     im2.convert('RGB').save(os.path.join(r'C:\\Users\\Big\\Documents\\01_Project\\202101_XAI\\01_data\\aug_original\\words', f'{word_dir[:-4]}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cce151f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openMarkedWordImg(word, train_test_val):\n",
    "    word_dir = os.path.join(f'C:/Users/01_data/general_{train_test_val}', word)\n",
    "    # 해당 단어 디렉토리에 있는 이미지 아무거나\n",
    "    word_file_name = random.sample(glob.glob(word_dir+'/*.jpg'), 1)\n",
    "    img = Image.open(word_file_name[0])\n",
    "    return img, img.size\n",
    "\n",
    "\n",
    "def openRegularWordImg(word):\n",
    "    # 마크 없는 단어는 한 종류 밖에 없으니,,,\n",
    "    word_file_name = os.path.join(r'C:\\Users\\01_data\\aug_original\\words', word+'.jpg')\n",
    "    img = Image.open(word_file_name)\n",
    "    return img, img.size\n",
    "\n",
    "\n",
    "def defineBox(num, arrangement):\n",
    "    # 행은 num으로 주어짐\n",
    "    next_y = 0\n",
    "    # i는 열 번호\n",
    "    for i in range(3):\n",
    "        # 각 열의 uppler left x 값을 구한다\n",
    "        arrangement[f'{num}{i+1}_x'] = 1\n",
    "        for j in range(i):\n",
    "            arrangement[f'{num}{i+1}_x'] += arrangement[f'{num}{j+1}_size'][0]\n",
    "        \n",
    "        # '해당 행의 가장 긴 세로 길이' + '이전 행의 uppler left y 값' = '다음 행의 upper left y'\n",
    "        next_y = max(next_y, arrangement[f'{num}_y']+arrangement[f'{num}{i+1}_size'][1])\n",
    "    arrangement[f'{num+1}_y'] = next_y\n",
    "    return arrangement\n",
    "\n",
    "\n",
    "def defineAnnot(marked_index, arrangement, max_x, max_y):\n",
    "    # <<format>> class x_center y_center width height (All normalized 0–1)\n",
    "    annot_txt = ''\n",
    "    \n",
    "    for index in marked_index:\n",
    "        # initial x_center, y_center\n",
    "        x_center = arrangement[f'{index}_x'] + 0.5*arrangement[f'{index}_size'][0]\n",
    "        y_center = arrangement[f'{index[0]}_y'] + 0.5*arrangement[f'{index}_size'][1]\n",
    "        \n",
    "        # ... after resizing\n",
    "        x_center = x_center / max_x\n",
    "        y_center = y_center /  max_y\n",
    "        width = arrangement[f'{index}_size'][0] / max_x\n",
    "        height = arrangement[f'{index}_size'][1] / max_y\n",
    "        \n",
    "        # annotation file의 각 줄에 해당하는 str을 만든다\n",
    "        line = f'0 {x_center} {y_center} {width} {height}\\n'\n",
    "        # 이미지 전체에 대한 str에 추가한다\n",
    "        annot_txt+=line\n",
    "        \n",
    "    return annot_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa014ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomTagGenerator(root_dir, iter_num, train_val_test):\n",
    "    for file_index in range(iter_num):\n",
    "        # 어떤 단어들을 쓸거냐면,,,\n",
    "        words_tag = random.sample(word_types, 9)\n",
    "        # 그 단어를 어디다 놓냐면,,,\n",
    "        random_index = random.sample(['11', '12', '13', '21', '22', '23', '31', '32', '33'], 9)\n",
    "        # arrangement = {'위치_인덱스ij': PIL_Image, 'ij_size': 이미지_사이즈, 'ij_x': upper_left_x, 'i_y': upper_left_y}\n",
    "        # marked index = [마킹된 '위치 인덱스'만 --to create annotation file]\n",
    "        arrangement, marked_index = {'1_y': 1}, []\n",
    "\n",
    "        # arrangement의 {'위치_인덱스': PIL_Image, 'size': 이미지_사이즈}\n",
    "        for i, key in enumerate(random_index):\n",
    "            # 랜덤으로 섞인 index 리스트의 앞에서 3개만 마킹된 단어 이미지\n",
    "            if i < 3:\n",
    "                arrangement[key], arrangement[f'{key}_size'] = openMarkedWordImg(words_tag[i], train_val_test)\n",
    "                marked_index.append(key)\n",
    "            # 나머진 마크 없는 단어 이미지\n",
    "            else:\n",
    "                arrangement[key], arrangement[f'{key}_size'] = openRegularWordImg(words_tag[i])\n",
    "\n",
    "        # arrangement의 {x': upper_left_x}\n",
    "        arrangement = defineBox(1, arrangement)\n",
    "        arrangement = defineBox(2, arrangement)\n",
    "        arrangement = defineBox(3, arrangement)    \n",
    "\n",
    "        # 일단 흰 바탕면 크게 만든다\n",
    "        blank_box = Image.new('RGB', (3000, 3000), (255,255,255))\n",
    "        # 흰 바탕면에 단어 이미지를 붙인다\n",
    "        # i는 행\n",
    "        for i in range(3):\n",
    "            # j는 열\n",
    "            for j in range(3):\n",
    "                im = arrangement[f'{i+1}{j+1}']\n",
    "                blank_box.paste(im, (arrangement[f'{i+1}{j+1}_x'], arrangement[f'{i+1}_y']))\n",
    "\n",
    "        # 단어 있는 부분만 잘라서 정사각형 이미지로 변신!\n",
    "        max_x = max(arrangement['13_x']+arrangement['13_size'][0], \\\n",
    "                    arrangement['23_x']+arrangement['23_size'][0], \\\n",
    "                    arrangement['33_x']+arrangement['33_size'][0])\n",
    "        max_y = arrangement['4_y']\n",
    "        blank_box = blank_box.crop((0,0,max_x,max_y)).resize((416, 416))\n",
    "\n",
    "        # 마크된 단어만 annotation 파일 만든다\n",
    "        annot_txt = defineAnnot(marked_index, arrangement, max_x, max_y)\n",
    "\n",
    "        # 저장\n",
    "        blank_box.save(f'{root_dir}/yolo_{file_index}.jpg')\n",
    "        f = open(f\"{root_dir}\\yolo_{file_index}.txt\", 'w')\n",
    "        f.write(annot_txt)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1da56ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dir = r'C:\\Users\\01_data\\general_yolo_train'\n",
    "iter_num_train = 17000\n",
    "randomTagGenerator(new_train_dir, iter_num_train, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a47c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "new_val_dir = r'C:\\Users\\01_data\\general_yolo_val'\n",
    "iter_num_val = 4000\n",
    "randomTagGenerator(new_val_dir, iter_num_val, 'val')\n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8aa514c2",
   "metadata": {},
   "source": [
    "### Word Classification model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a280009a",
   "metadata": {},
   "source": [
    "Import modules and set up a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126f0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Using cached tensorflow_addons-0.16.1-cp38-cp38-win_amd64.whl (755 kB)\n",
      "Requirement already satisfied: typeguard>=2.7 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Installing collected packages: tensorflow_addons\n",
      "Successfully installed tensorflow_addons-0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66962a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import glob, random, warnings\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('TensorFlow Version ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df651c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 설정\n",
    "\n",
    "# tf.config.list_logical_devices()\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd68871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU') \n",
    "\n",
    "# if gpus: \n",
    "#     try: \n",
    "#         # Currently, memory growth needs to be the same across GPUs \n",
    "#         for gpu in gpus: \n",
    "#             tf.config.experimental.set_virtual_device_configuration(gpus[-1],\n",
    "#                                                                     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "# #             tf.config.experimental.set_memory_growth(gpu, True) \n",
    "#             logical_gpus = tf.config.experimental.list_logical_devices('GPU') \n",
    "#             print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\") \n",
    "            \n",
    "#     except RuntimeError as e: \n",
    "#         # Memory growth must be set before GPUs have been initialized \n",
    "#         print(e)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# # gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# # if gpus:\n",
    "# #     try:\n",
    "# #         tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# #     except RuntimeError as e:\n",
    "# #         print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ff79b1a",
   "metadata": {},
   "source": [
    "Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e83bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = r'C:\\Users\\CAD-06\\vit\\word_train'\n",
    "VAL_PATH = r'C:\\Users\\CAD-06\\vit\\word_val'\n",
    "TEST_PATH = r'C:\\Users\\CAD-06\\vit\\word_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6075cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "# 32에서 16으로 바꿈\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5751ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '1', 1: '2', 2: '3', 3: '4', 4: '가구', 5: '간섭', 6: '개폐', 7: '거실', 8: '고정', 9: '공', 10: '과다', 11: '기타', 12: '누락', 13: '단차', 14: '대피공간', 15: '도배', 16: '도장', 17: '드레스룸', 18: '들뜸', 19: '마감', 20: '마감판', 21: '면불량', 22: '문', 23: '미시공', 24: '미흡', 25: '바닥', 26: '발코니', 27: '벽', 28: '부', 29: '불량', 30: '붙박이장', 31: '석고', 32: '세부위치', 33: '세탁실', 34: '수직수평', 35: '스크레치', 36: '실링', 37: '실외기실', 38: '안', 39: '알', 40: '오염', 41: '욕실', 42: '유격', 43: '이격', 44: '이음불량', 45: '작동', 46: '잠금', 47: '전등', 48: '주방', 49: '줄눈', 50: '찍힘', 51: '창호', 52: '천장', 53: '침', 54: '코킹', 55: '콘센트', 56: '타일', 57: '태움', 58: '틀', 59: '틈새', 60: '파손', 61: '파우더', 62: '팬트리', 63: '현관', 64: '확인'}\n"
     ]
    }
   ],
   "source": [
    "words = ['1', '2', '3', '4', '가구', '간섭', '개폐', '거실', '고정', '공', '과다', '기타', '누락', '단차', '대피공간', '도배', '도장', '드레스룸', '들뜸', '마감', '마감판', '면불량', '문', '미시공', '미흡', '바닥', '발코니', '벽', '부', '불량', '붙박이장', '석고', '세부위치', '세탁실', '수직수평', '스크레치', '실링', '실외기실', '안', '알', '오염', '욕실', '유격', '이격', '이음불량', '작동', '잠금', '전등', '주방', '줄눈', '찍힘', '창호', '천장', '침', '코킹', '콘센트', '타일', '태움', '틀', '틈새', '파손', '파우더', '팬트리', '현관', '확인']\n",
    "classes = {}\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    classes[i] = word\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c17c3846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 73872 images belonging to 65 classes.\n",
      "Found 17967 images belonging to 65 classes.\n",
      "Found 1381 images belonging to 65 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 0,\n",
    "                                                          samplewise_center = True,\n",
    "                                                          samplewise_std_normalization = True)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_dataframe\n",
    "train_gen = datagen.flow_from_directory(directory = TRAIN_PATH,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = IMAGE_SIZE)\n",
    "\n",
    "val_gen = datagen.flow_from_directory(directory = VAL_PATH,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = IMAGE_SIZE)\n",
    "\n",
    "test_gen = datagen.flow_from_directory(directory = TEST_PATH,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = IMAGE_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "707196e9",
   "metadata": {},
   "source": [
    "Download pre-trained ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7096ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18440914",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet vit-keras\n",
    "\n",
    "from vit_keras import vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3696aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\vit\\lib\\site-packages\\vit_keras\\utils.py:81: UserWarning: Resizing position embeddings from 12, 12 to 7, 7\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vit_model = vit.vit_b32(\n",
    "        image_size = IMAGE_SIZE,\n",
    "        activation = 'softmax',\n",
    "        pretrained = True,\n",
    "        include_top = False,\n",
    "        pretrained_top = False,\n",
    "        classes = 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b4ce1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " embedding (Conv2D)          (None, 7, 7, 768)         2360064   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 49, 768)           0         \n",
      "                                                                 \n",
      " class_token (ClassToken)    (None, 50, 768)           768       \n",
      "                                                                 \n",
      " Transformer/posembed_input   (None, 50, 768)          38400     \n",
      " (AddPositionEmbs)                                               \n",
      "                                                                 \n",
      " Transformer/encoderblock_0   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_1   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_2   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_3   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_4   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_5   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_6   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_7   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_8   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_9   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_10  ((None, 50, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_11  ((None, 50, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoder_norm (L  (None, 50, 768)          1536      \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " ExtractToken (Lambda)       (None, 768)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 768)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 768)              3072      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11)                8459      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 11)               44        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 65)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,467,587\n",
      "Trainable params: 87,466,029\n",
      "Non-trainable params: 1,558\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "out = tf.keras.layers.Flatten()(vit_model.output)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(11, activation = tfa.activations.gelu)(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(65, 'softmax')(out)\n",
    "model = tf.keras.Model(inputs = vit_model.input, outputs = out)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b022fa19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 2.9538 - accuracy: 0.7120\n",
      "Epoch 1: val_accuracy improved from -inf to 0.98323, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14577s 3s/step - loss: 2.9538 - accuracy: 0.7120 - val_loss: 2.0485 - val_accuracy: 0.9832 - lr: 1.0000e-04\n",
      "Epoch 2/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.8904 - accuracy: 0.9710\n",
      "Epoch 2: val_accuracy improved from 0.98323 to 0.99070, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14109s 3s/step - loss: 1.8904 - accuracy: 0.9710 - val_loss: 1.5907 - val_accuracy: 0.9907 - lr: 1.0000e-04\n",
      "Epoch 3/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.6582 - accuracy: 0.9809\n",
      "Epoch 3: val_accuracy improved from 0.99070 to 0.99326, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14122s 3s/step - loss: 1.6582 - accuracy: 0.9809 - val_loss: 1.5866 - val_accuracy: 0.9933 - lr: 1.0000e-04\n",
      "Epoch 4/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.6281 - accuracy: 0.9841\n",
      "Epoch 4: val_accuracy did not improve from 0.99326\n",
      "4617/4617 [==============================] - 14125s 3s/step - loss: 1.6281 - accuracy: 0.9841 - val_loss: 1.5802 - val_accuracy: 0.9924 - lr: 1.0000e-04\n",
      "Epoch 5/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.6159 - accuracy: 0.9852\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.99326\n",
      "4617/4617 [==============================] - 14218s 3s/step - loss: 1.6159 - accuracy: 0.9852 - val_loss: 1.5772 - val_accuracy: 0.9914 - lr: 1.0000e-04\n",
      "Epoch 6/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5788 - accuracy: 0.9953\n",
      "Epoch 6: val_accuracy improved from 0.99326 to 0.99694, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14112s 3s/step - loss: 1.5788 - accuracy: 0.9953 - val_loss: 1.5562 - val_accuracy: 0.9969 - lr: 2.0000e-05\n",
      "Epoch 7/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5725 - accuracy: 0.9970\n",
      "Epoch 7: val_accuracy did not improve from 0.99694\n",
      "4617/4617 [==============================] - 14101s 3s/step - loss: 1.5725 - accuracy: 0.9970 - val_loss: 1.5565 - val_accuracy: 0.9969 - lr: 2.0000e-05\n",
      "Epoch 8/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5688 - accuracy: 0.9978\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.99694 to 0.99699, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14091s 3s/step - loss: 1.5688 - accuracy: 0.9978 - val_loss: 1.5573 - val_accuracy: 0.9970 - lr: 2.0000e-05\n",
      "Epoch 9/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5653 - accuracy: 0.9989\n",
      "Epoch 9: val_accuracy improved from 0.99699 to 0.99794, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14111s 3s/step - loss: 1.5653 - accuracy: 0.9989 - val_loss: 1.5537 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 10/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5639 - accuracy: 0.9992\n",
      "Epoch 10: val_accuracy did not improve from 0.99794\n",
      "4617/4617 [==============================] - 14135s 3s/step - loss: 1.5639 - accuracy: 0.9992 - val_loss: 1.5513 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 11/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5634 - accuracy: 0.9994\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.99794\n",
      "4617/4617 [==============================] - 14108s 3s/step - loss: 1.5634 - accuracy: 0.9994 - val_loss: 1.5528 - val_accuracy: 0.9978 - lr: 4.0000e-06\n"
     ]
    }
   ],
   "source": [
    "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = 1e-4)\n",
    "\n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
    "                                                 factor = 0.2,\n",
    "                                                 patience = 2,\n",
    "                                                 verbose = 1,\n",
    "                                                 min_delta = 1e-4,\n",
    "                                                 min_lr = 1e-6,\n",
    "                                                 mode = 'max')\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                 min_delta = 1e-4,\n",
    "                                                 patience = 5,\n",
    "                                                 mode = 'max',\n",
    "                                                 restore_best_weights = True,\n",
    "                                                 verbose = 1)\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n",
    "                                                  monitor = 'val_accuracy', \n",
    "                                                  verbose = 1, \n",
    "                                                  save_best_only = True,\n",
    "                                                  save_weights_only = True,\n",
    "                                                  mode = 'max')\n",
    "\n",
    "callbacks = [earlystopping, reduce_lr, checkpointer]\n",
    "\n",
    "STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n",
    "STEP_SIZE_VALID = val_gen.n // val_gen.batch_size\n",
    "\n",
    "model.fit(x = train_gen,\n",
    "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "          validation_data = val_gen,\n",
    "          validation_steps = STEP_SIZE_VALID,\n",
    "          epochs = EPOCHS,\n",
    "          shuffle = True,\n",
    "          callbacks = callbacks)\n",
    "\n",
    "model.save('word_classification_model2.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3f469c3",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940af601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet vit-keras\n",
    "\n",
    "from vit_keras import vit\n",
    "from keras.models import load_model\n",
    "\n",
    "my_model = load_model('word_classification_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "304e1f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 224, 224, 3)\n",
      "{'1': 0, '2': 1, '3': 2, '4': 3, '가구': 4, '간섭': 5, '개폐': 6, '거실': 7, '고정': 8, '공': 9, '과다': 10, '기타': 11, '누락': 12, '단차': 13, '대피공간': 14, '도배': 15, '도장': 16, '드레스룸': 17, '들뜸': 18, '마감': 19, '마감판': 20, '면불량': 21, '문': 22, '미시공': 23, '미흡': 24, '바닥': 25, '발코니': 26, '벽': 27, '부': 28, '불량': 29, '붙박이장': 30, '석고': 31, '세부위치': 32, '세탁실': 33, '수직수평': 34, '스크레치': 35, '실링': 36, '실외기실': 37, '안': 38, '알': 39, '오염': 40, '욕실': 41, '유격': 42, '이격': 43, '이음불량': 44, '작동': 45, '잠금': 46, '전등': 47, '주방': 48, '줄눈': 49, '찍힘': 50, '창호': 51, '천장': 52, '침': 53, '코킹': 54, '콘센트': 55, '타일': 56, '태움': 57, '틀': 58, '틈새': 59, '파손': 60, '파우더': 61, '팬트리': 62, '현관': 63, '확인': 64} \n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.5393 - accuracy: 1.0000\n",
      "[1.5393190383911133, 1.0]\n"
     ]
    }
   ],
   "source": [
    "test_images, test_labels = next(iter(test_gen))\n",
    "labels = train_gen.class_indices\n",
    "\n",
    "print(test_images.shape)\n",
    "print(labels, '\\n')\n",
    "\n",
    "results = my_model.evaluate(test_images, test_labels, batch_size=BATCH_SIZE)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526207ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 65)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         1\n",
      "          17       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          27       1.00      1.00      1.00         1\n",
      "          28       1.00      1.00      1.00         1\n",
      "          29       1.00      1.00      1.00         2\n",
      "          33       1.00      1.00      1.00         1\n",
      "          41       1.00      1.00      1.00         1\n",
      "          48       1.00      1.00      1.00         2\n",
      "          50       1.00      1.00      1.00         1\n",
      "          51       1.00      1.00      1.00         1\n",
      "          62       1.00      1.00      1.00         1\n",
      "          64       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        16\n",
      "   macro avg       1.00      1.00      1.00        16\n",
      "weighted avg       1.00      1.00      1.00        16\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = my_model.predict(test_images)\n",
    "print(predictions.shape)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(test_labels, axis=1) \n",
    "\n",
    "confusionmatrix = confusion_matrix(true_classes, predicted_classes)\n",
    "plt.figure(figsize = (8, 8))\n",
    "sns.heatmap(confusionmatrix, cmap = 'Blues', annot = True, cbar = True)\n",
    "\n",
    "print(classification_report(true_classes, predicted_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
