{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce14b5e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp38-cp38-win_amd64.whl (438.0 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1612fd7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12806282963372238751\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6940786688\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8229374723827501161\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c546c5e6",
   "metadata": {},
   "source": [
    "## Data Augmentation Code\n",
    "https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#gaussianblur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43ffde1",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03cd31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "print('torch version:', torch.__version__)\n",
    "\n",
    "word_types = ['1', '2', '3', '4', '가구', '간섭', '개폐', '거실', '고정', '공', '과다', '기타', '누락', '단차', '대피공간', '도배', '도장', '드레스룸', '들뜸', '마감', '마감판', '면불량', '문', '미시공', '미흡', '바닥', '발코니', '벽', '부', '불량', '붙박이장', '석고', '세부위치', '세탁실', '수직수평', '스크레치', '실링', '실외기실', '안', '알', '오염', '욕실', '유격', '이격', '이음불량', '작동', '잠금', '전등', '주방', '줄눈', '찍힘', '창호', '천장', '침', '코킹', '콘센트', '타일', '태움', '틀', '틈새', '파손', '파우더', '팬트리', '현관', '확인']\n",
    "ROOT_DIR = r'C:\\Users\\01_data\\aug_original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3863b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fa0001f",
   "metadata": {},
   "source": [
    "### Tick Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb592608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original images: 65\n",
      "Example of file name: 1.jpg\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = r'C:\\Users\\01_data\\aug_original'\n",
    "original_list = os.listdir(os.path.join(ROOT_DIR,'words/'))\n",
    "\n",
    "print('Number of original images:', len(original_list))\n",
    "print('Example of file name:', original_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7efdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# white = Image.new('RGB', (1000, 1000), (255,255,255))\n",
    "# white.save(os.path.join(ROOT_DIR,'white.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37a94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circle augmentation\n",
    "for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'tick'))):\n",
    "    \n",
    "    # translation, scaling by Affine transfrom\n",
    "    def AffineTransform(img_dir, new_dir):\n",
    "        orig_img = Image.open(img_dir)\n",
    "        affine_transformer = T.RandomAffine(degrees=(0, 0), translate=(0.2, 0.2), scale=(0.5, 1.5))\n",
    "        affine_imgs = [affine_transformer(orig_img) for _ in range(7)]\n",
    "        for i, affine_img in enumerate(affine_imgs):\n",
    "            affine_img.save(f'{new_dir}_a_{i}.png')\n",
    "    \n",
    "    full_dir = os.path.join(ROOT_DIR, 'tick', circle_dir)\n",
    "    new_dir = os.path.join(ROOT_DIR, 'aug_tick', circle_dir[:-4])\n",
    "    \n",
    "    AffineTransform(full_dir, new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99538231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.png is done\n",
      "2.png is done\n",
      "3.png is done\n",
      "4.png is done\n",
      "가구.png is done\n",
      "간섭.png is done\n",
      "개폐.png is done\n",
      "거실.png is done\n",
      "고정.png is done\n",
      "공.png is done\n",
      "과다.png is done\n",
      "기타.png is done\n",
      "누락.png is done\n",
      "단차.png is done\n",
      "대피공간.png is done\n",
      "도배.png is done\n",
      "도장.png is done\n",
      "드레스룸.png is done\n",
      "들뜸.png is done\n",
      "마감.png is done\n",
      "마감판.png is done\n",
      "면불량.png is done\n",
      "문.png is done\n",
      "미시공.png is done\n",
      "미흡.png is done\n",
      "바닥.png is done\n",
      "발코니.png is done\n",
      "벽.png is done\n",
      "부.png is done\n",
      "불량.png is done\n",
      "붙박이장.png is done\n",
      "석고.png is done\n",
      "세부위치.png is done\n",
      "세탁실.png is done\n",
      "수직수평.png is done\n",
      "스크레치.png is done\n",
      "실링.png is done\n",
      "실외기실.png is done\n",
      "안.png is done\n",
      "알.png is done\n",
      "오염.png is done\n",
      "욕실.png is done\n",
      "유격.png is done\n",
      "이격.png is done\n",
      "이음불량.png is done\n",
      "작동.png is done\n",
      "잠금.png is done\n",
      "전등.png is done\n",
      "주방.png is done\n",
      "줄눈.png is done\n",
      "찍힘.png is done\n",
      "창호.png is done\n",
      "천장.png is done\n",
      "침.png is done\n",
      "코킹.png is done\n",
      "콘센트.png is done\n",
      "타일.png is done\n",
      "태움.png is done\n",
      "틀.png is done\n",
      "틈새.png is done\n",
      "파손.png is done\n",
      "파우더.png is done\n",
      "팬트리.png is done\n",
      "현관.png is done\n",
      "확인.png is done\n",
      "All finished\n"
     ]
    }
   ],
   "source": [
    "# Read image\n",
    "def add_circle(word_dir):\n",
    "    word = Image.open(os.path.join(ROOT_DIR, 'words', word_dir))\n",
    "    \n",
    "    for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'aug_tick'))):\n",
    "        # Resize the circle image to fit circle\n",
    "        word2 = word.copy()\n",
    "        w, h = word2.size\n",
    "        \n",
    "        circle = Image.open(os.path.join(ROOT_DIR, 'aug_tick', circle_dir))\n",
    "        circle2 = circle.resize((w, h))\n",
    "        w2, h2 = circle2.size\n",
    "\n",
    "#         if w >= h:\n",
    "#             circle3 = circle2.crop((0, h2/2-h/2, w, h2/2+h/2))\n",
    "#         else:\n",
    "#             circle3 = circle2.crop((w2/2-w/2, 0, w2/2+w/2, h))\n",
    "#         circle3 = circle3.resize((w, h))\n",
    "\n",
    "        # paste circle to word image\n",
    "#         im = Image.alpha_composite(word2.convert('RGBA'), circle3.convert('RGBA'))\n",
    "        im = Image.alpha_composite(word2.convert('RGBA'), circle2.convert('RGBA'))\n",
    "        \n",
    "        # Make background white\n",
    "        white = Image.open(os.path.join(ROOT_DIR, 'white.jpg'))\n",
    "        white2 = white.resize(im.size)\n",
    "        \n",
    "        im2 = Image.alpha_composite(white2.convert('RGBA'), im)\n",
    "        im2.convert('RGB').save(f'{ROOT_DIR}/aug_tick_word/{word_dir[:-4]}_{i}.jpg')\n",
    "        \n",
    "        \n",
    "for word_dir in os.listdir(os.path.join(ROOT_DIR, 'words')):\n",
    "    add_circle(word_dir)\n",
    "    print(word_dir, 'is done')\n",
    "print('All finished')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9d39d94",
   "metadata": {},
   "source": [
    "### Random Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bddcd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianBlur(img_dir, new_dir):\n",
    "    orig_img = Image.open(img_dir)\n",
    "    blurrer = T.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 10))   \n",
    "    blurred_imgs = [blurrer(orig_img) for _ in range(3)]\n",
    "    for i, blurred_img in enumerate(blurred_imgs):\n",
    "        blurred_img.save(new_dir+'_g_'+str(i)+'.jpg')\n",
    "\n",
    "    \n",
    "def PerspectiveTransform(img_dir, new_dir):\n",
    "    orig_img = Image.open(img_dir)\n",
    "    perspective_transformer = T.RandomPerspective(distortion_scale=0.5, p=1.0, fill=(255,255,255))\n",
    "    perspective_imgs = [perspective_transformer(orig_img) for _ in range(5)]\n",
    "    for i, perspective_img in enumerate(perspective_imgs):\n",
    "        perspective_img.save(new_dir+'_p_'+str(i)+'.jpg')\n",
    "\n",
    "    \n",
    "# def AffineTransform(img_dir, new_dir):\n",
    "#     orig_img = Image.open(img_dir)\n",
    "#     affine_transformer = T.RandomAffine(degrees=(-20, 20), translate=(0.2, 0.2), scale=(0.5, 2))\n",
    "#     affine_imgs = [affine_transformer(orig_img) for _ in range(7)]\n",
    "#     for i, affine_img in enumerate(affine_imgs):\n",
    "#         affine_img.save(new_dir+'_a_'+str(i)+'.jpg')\n",
    "        \n",
    "        \n",
    "# def Brightness(img_dir, new_dir):\n",
    "#     orig_img = Image.open(img_dir)\n",
    "#     for i in range(5):\n",
    "#         brightness_adjuster = T.functional.adjust_brightness(brightness_factor=np.random.rand(1))\n",
    "#         brightness_img = brightness_adjuster(orig_img)\n",
    "#         brightness_img.save(new_dir+'_b_'+str(i)+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ceb400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4550/4550 [07:47<00:00,  9.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_tick_word/')):\n",
    "    img_dir = ROOT_DIR+'/aug_tick_word/'+img_file\n",
    "    \n",
    "    new_dir_g = ROOT_DIR+'/aug_tick_word2/GaussianBlurred/'+img_file[:-4]\n",
    "    GaussianBlur(img_dir, new_dir_g) \n",
    "    \n",
    "#     new_dir_p = ROOT_DIR+'aug_result/PerspectiveTransformed/'+img_file[:-4]\n",
    "#     PerspectiveTransform(img_dir, new_dir_p)\n",
    "    \n",
    "#     new_dir_a = ROOT_DIR+'aug_result/AffineTransformed/'+img_file[:-4]\n",
    "#     AffineTransform(img_dir, new_dir_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09571d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13650/13650 [24:12<00:00,  9.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_tick_word2/GaussianBlurred/')):\n",
    "    img_dir = ROOT_DIR+'/aug_tick_word2/GaussianBlurred/'+img_file\n",
    "    \n",
    "    new_dir_p = ROOT_DIR+'/aug_tick_word2/PerspectiveTransformed/'+img_file[:-4]\n",
    "    PerspectiveTransform(img_dir, new_dir_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1366b068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gaussian blurred file: 13650\n",
      "Number of perspective transformed file: 68250\n"
     ]
    }
   ],
   "source": [
    "gaussian_dir = ROOT_DIR+'/aug_tick_word2/GaussianBlurred/'\n",
    "perspective_dir = ROOT_DIR+'/aug_tick_word2/PerspectiveTransformed/'\n",
    "# affine_dir = ROOT_DIR+'aug_result/AffineTransformed/'\n",
    "# brightness_dir = ROOT_DIR+'aug_result/Brightness/'\n",
    "\n",
    "print('Number of gaussian blurred file:', len(os.listdir(gaussian_dir)))\n",
    "print('Number of perspective transformed file:', len(os.listdir(perspective_dir)))\n",
    "# print('Number of affine transformed file:', len(os.listdir(affine_dir)))\n",
    "# print('total:', 650+len(os.listdir(gaussian_dir))+len(os.listdir(perspective_dir))+len(os.listdir(affine_dir)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9bd217b",
   "metadata": {},
   "source": [
    "### Circle Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37a51538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circle augmentation\n",
    "for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'circle'))):\n",
    "    \n",
    "    # translation, scaling by Affine transfrom\n",
    "    def AffineTransform(img_dir, new_dir):\n",
    "        orig_img = Image.open(img_dir)\n",
    "        affine_transformer = T.RandomAffine(degrees=(0, 0), translate=(0.2, 0.2), scale=(0.5, 1.5))\n",
    "        affine_imgs = [affine_transformer(orig_img) for _ in range(7)]\n",
    "        for i, affine_img in enumerate(affine_imgs):\n",
    "            affine_img.save(f'{new_dir}_cir_a_{i}.png')\n",
    "    \n",
    "    full_dir = os.path.join(ROOT_DIR, 'circle', circle_dir)\n",
    "    new_dir = os.path.join(ROOT_DIR, 'aug_circle', circle_dir[:-4])\n",
    "    \n",
    "    AffineTransform(full_dir, new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4381ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.png is done\n",
      "2.png is done\n",
      "3.png is done\n",
      "4.png is done\n",
      "가구.png is done\n",
      "간섭.png is done\n",
      "개폐.png is done\n",
      "거실.png is done\n",
      "고정.png is done\n",
      "공.png is done\n",
      "과다.png is done\n",
      "기타.png is done\n",
      "누락.png is done\n",
      "단차.png is done\n",
      "대피공간.png is done\n",
      "도배.png is done\n",
      "도장.png is done\n",
      "드레스룸.png is done\n",
      "들뜸.png is done\n",
      "마감.png is done\n",
      "마감판.png is done\n",
      "면불량.png is done\n",
      "문.png is done\n",
      "미시공.png is done\n",
      "미흡.png is done\n",
      "바닥.png is done\n",
      "발코니.png is done\n",
      "벽.png is done\n",
      "부.png is done\n",
      "불량.png is done\n",
      "붙박이장.png is done\n",
      "석고.png is done\n",
      "세부위치.png is done\n",
      "세탁실.png is done\n",
      "수직수평.png is done\n",
      "스크레치.png is done\n",
      "실링.png is done\n",
      "실외기실.png is done\n",
      "안.png is done\n",
      "알.png is done\n",
      "오염.png is done\n",
      "욕실.png is done\n",
      "유격.png is done\n",
      "이격.png is done\n",
      "이음불량.png is done\n",
      "작동.png is done\n",
      "잠금.png is done\n",
      "전등.png is done\n",
      "주방.png is done\n",
      "줄눈.png is done\n",
      "찍힘.png is done\n",
      "창호.png is done\n",
      "천장.png is done\n",
      "침.png is done\n",
      "코킹.png is done\n",
      "콘센트.png is done\n",
      "타일.png is done\n",
      "태움.png is done\n",
      "틀.png is done\n",
      "틈새.png is done\n",
      "파손.png is done\n",
      "파우더.png is done\n",
      "팬트리.png is done\n",
      "현관.png is done\n",
      "확인.png is done\n",
      "All finished\n"
     ]
    }
   ],
   "source": [
    "# Read image\n",
    "def add_circle(word_dir):\n",
    "    word = Image.open(os.path.join(ROOT_DIR, 'words', word_dir))\n",
    "    \n",
    "    for i, circle_dir in enumerate(os.listdir(os.path.join(ROOT_DIR, 'aug_circle'))):\n",
    "        # Resize the circle image to fit circle\n",
    "        word2 = word.copy()\n",
    "        w, h = word2.size\n",
    "        \n",
    "        circle = Image.open(os.path.join(ROOT_DIR, 'aug_circle', circle_dir))\n",
    "        circle2 = circle.resize((w, h))\n",
    "        w2, h2 = circle2.size\n",
    "\n",
    "        im = Image.alpha_composite(word2.convert('RGBA'), circle2.convert('RGBA'))\n",
    "        \n",
    "        # Make background white\n",
    "        white = Image.open(os.path.join(ROOT_DIR, 'white.jpg'))\n",
    "        white2 = white.resize(im.size)\n",
    "        \n",
    "        im2 = Image.alpha_composite(white2.convert('RGBA'), im)\n",
    "        im2.convert('RGB').save(f'{ROOT_DIR}/aug_circle_word/{word_dir[:-4]}_cir_{i}.jpg')\n",
    "        \n",
    "        \n",
    "for word_dir in os.listdir(os.path.join(ROOT_DIR, 'words')):\n",
    "    add_circle(word_dir)\n",
    "    print(word_dir, 'is done')\n",
    "print('All finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "038cff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 13650/13650 [17:59<00:00, 12.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_circle_word/')):\n",
    "    img_dir = ROOT_DIR+'/aug_circle_word/'+img_file\n",
    "    \n",
    "    new_dir_g = ROOT_DIR+'/aug_circle_word2/GaussianBlurred/'+img_file[:-4]\n",
    "    GaussianBlur(img_dir, new_dir_g) \n",
    "    \n",
    "for img_file in tqdm(os.listdir(ROOT_DIR+'/aug_circle_word2/GaussianBlurred/')):\n",
    "    img_dir = ROOT_DIR+'/aug_circle_word2/GaussianBlurred/'+img_file\n",
    "    \n",
    "    new_dir_p = ROOT_DIR+'/aug_circle_word2/PerspectiveTransformed/'+img_file[:-4]\n",
    "    PerspectiveTransform(img_dir, new_dir_p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bfac2fb",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53c675ca",
   "metadata": {},
   "source": [
    "Create folders for each semantic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5144dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "word_types = ['1', '2', '3', '4', '가구', '간섭', '개폐', '거실', '고정', '공', '과다', '기타', '누락', '단차', '대피공간', '도배', '도장', '드레스룸', '들뜸', '마감', '마감판', '면불량', '문', '미시공', '미흡', '바닥', '발코니', '벽', '부', '불량', '붙박이장', '석고', '세부위치', '세탁실', '수직수평', '스크레치', '실링', '실외기실', '안', '알', '오염', '욕실', '유격', '이격', '이음불량', '작동', '잠금', '전등', '주방', '줄눈', '찍힘', '창호', '천장', '침', '코킹', '콘센트', '타일', '태움', '틀', '틈새', '파손', '파우더', '팬트리', '현관', '확인']\n",
    "\n",
    "for word in word_types:\n",
    "    dd_directory = os.path.join('C:/Users/01_data/general_train', word)\n",
    "    os.makedirs(dd_directory)\n",
    "    \n",
    "for word in word_types:\n",
    "    dd_directory = os.path.join('C:/Users/01_data/general_val', word)\n",
    "    os.makedirs(dd_directory)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed7ecb35",
   "metadata": {},
   "source": [
    "1) Real world images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d65477cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SaveCropped(txt_dir, start_index):\n",
    "#     with open(txt_dir, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     DIR_CROPPED = '/home/deep-text-recognition-benchmark/new_data'\n",
    "#     DIR_NEW_TRAIN = '/home/word_train/'\n",
    "#     DIR_NEW_VAL = '/home/word_val/'\n",
    "#     DIR_NEW_TEST = '/home/word_test/'\n",
    "    \n",
    "#     lines222 = lines[start_index:]\n",
    "\n",
    "#     for i, line in enumerate(lines222):\n",
    "#         data = line.strip().split('\t')\n",
    "#         data_dir_full = DIR_CROPPED + data[0][10:]\n",
    "        \n",
    "#         index = data[0].rfind('/')\n",
    "#         data_name = data[0][index:]\n",
    "#         data_type = data[1]\n",
    "        \n",
    "#         if i < len(lines222)*0.8:\n",
    "#             new_dir_full = DIR_NEW_TRAIN + data_type + data_name\n",
    "#         elif i < len(lines222)*0.9:\n",
    "#             new_dir_full = DIR_NEW_VAL + data_type + data_name\n",
    "#         else:\n",
    "#             new_dir_full = DIR_NEW_TEST + data_type + data_name\n",
    "\n",
    "#         img = Image.open(data_dir_full)\n",
    "#         img.save(new_dir_full)\n",
    "        \n",
    "# SaveCropped('/home/deep-text-recognition-benchmark/new_data/gt_validation.txt', 5309)\n",
    "# SaveCropped('/home/deep-text-recognition-benchmark/new_data/gt_train.txt', 25863)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a7df52f",
   "metadata": {},
   "source": [
    "2) Synthesized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "886e106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Big/Documents/01_Project/202101_XAI/01_data/aug_original/aug_tick_word\\1_0.jpg\n"
     ]
    }
   ],
   "source": [
    "hello = {}\n",
    "for word_type in word_types:\n",
    "    hello[word_type] = []\n",
    "\n",
    "def LetmeClassify(root_dir):\n",
    "    for file_name in glob.glob(root_dir+'/*.jpg'):\n",
    "        short_name = file_name[file_name.rfind('\\\\')+1:]\n",
    "        word_type = short_name[:short_name.find('_')]\n",
    "        hello[word_type].append(file_name)\n",
    "        \n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_tick_word')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_tick_word2/GaussianBlurred')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_tick_word2/PerspectiveTransformed')\n",
    "\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_star_word')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_star_word2/GaussianBlurred')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_star_word2/PerspectiveTransformed')\n",
    "\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_circle_word')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_circle_word2/GaussianBlurred')\n",
    "LetmeClassify('C:/Users/01_data/aug_original/aug_circle_word2/PerspectiveTransformed')\n",
    "\n",
    "print(hello['1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd59a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3990\n"
     ]
    }
   ],
   "source": [
    "print(len(hello['1']))\n",
    "# print(len(hello['2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af602979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: 1\n",
      "Finished: 2\n",
      "Finished: 3\n",
      "Finished: 4\n",
      "Finished: 가구\n",
      "Finished: 간섭\n",
      "Finished: 개폐\n",
      "Finished: 거실\n",
      "Finished: 고정\n",
      "Finished: 공\n",
      "Finished: 과다\n",
      "Finished: 기타\n",
      "Finished: 누락\n",
      "Finished: 단차\n",
      "Finished: 대피공간\n",
      "Finished: 도배\n",
      "Finished: 도장\n",
      "Finished: 드레스룸\n",
      "Finished: 들뜸\n",
      "Finished: 마감\n",
      "Finished: 마감판\n",
      "Finished: 면불량\n",
      "Finished: 문\n",
      "Finished: 미시공\n",
      "Finished: 미흡\n",
      "Finished: 바닥\n",
      "Finished: 발코니\n",
      "Finished: 벽\n",
      "Finished: 부\n",
      "Finished: 불량\n",
      "Finished: 붙박이장\n",
      "Finished: 석고\n",
      "Finished: 세부위치\n",
      "Finished: 세탁실\n",
      "Finished: 수직수평\n",
      "Finished: 스크레치\n",
      "Finished: 실링\n",
      "Finished: 실외기실\n",
      "Finished: 안\n",
      "Finished: 알\n",
      "Finished: 오염\n",
      "Finished: 욕실\n",
      "Finished: 유격\n",
      "Finished: 이격\n",
      "Finished: 이음불량\n",
      "Finished: 작동\n",
      "Finished: 잠금\n",
      "Finished: 전등\n",
      "Finished: 주방\n",
      "Finished: 줄눈\n",
      "Finished: 찍힘\n",
      "Finished: 창호\n",
      "Finished: 천장\n",
      "Finished: 침\n",
      "Finished: 코킹\n",
      "Finished: 콘센트\n",
      "Finished: 타일\n",
      "Finished: 태움\n",
      "Finished: 틀\n",
      "Finished: 틈새\n",
      "Finished: 파손\n",
      "Finished: 파우더\n",
      "Finished: 팬트리\n",
      "Finished: 현관\n",
      "Finished: 확인\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "DIR_NEW_TRAIN = r'C:\\Users\\01_data\\general_train'\n",
    "DIR_NEW_VAL = r'C:\\Users\\01_data\\general_val'\n",
    "DIR_NEW_TEST = r'C:\\Users\\01_data\\general_test'\n",
    "\n",
    "import random\n",
    "\n",
    "for word_type in word_types:\n",
    "    file_name_list = random.sample(list(hello[word_type]), len(hello['1']))\n",
    "    \n",
    "    for i, file_name in enumerate(file_name_list):\n",
    "        short_name = file_name[file_name.rfind('\\\\')+1:]\n",
    "        img = Image.open(file_name)\n",
    "        \n",
    "        if i < 0.8*len(hello['1']):\n",
    "            img.save(os.path.join(DIR_NEW_TRAIN, word_type, short_name))\n",
    "        else:\n",
    "            img.save(os.path.join(DIR_NEW_VAL, word_type, short_name))\n",
    "            \n",
    "    print(f'Finished: {word_type}')\n",
    "print('done')        \n",
    "#         if i < 0.6*2450:\n",
    "#             img.save(os.path.join(DIR_NEW_TRAIN, word_type, short_name))\n",
    "#         elif i < 0.8*2450:\n",
    "#             img.save(os.path.join(DIR_NEW_VAL, word_type, short_name))\n",
    "#         else:\n",
    "#             img.save(os.path.join(DIR_NEW_TEST, word_type, short_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78d579e2",
   "metadata": {},
   "source": [
    "Circle - save at the corresponding folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fe908f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hello = {}\n",
    "# for word_type in word_types:\n",
    "#     hello[word_type] = []\n",
    "\n",
    "# LetmeClassify('C:/Users/Big/Documents/01_Project/202101_XAI/01_data/aug_original/aug_circle_word')\n",
    "# LetmeClassify('C:/Users/Big/Documents/01_Project/202101_XAI/01_data/aug_original/aug_circle_word2/GaussianBlurred')\n",
    "# LetmeClassify('C:/Users/Big/Documents/01_Project/202101_XAI/01_data/aug_original/aug_circle_word2/PerspectiveTransformed')\n",
    "    \n",
    "# for word_type in word_types:\n",
    "#     file_name_list = random.sample(list(hello[word_type]), 1330)\n",
    "    \n",
    "#     for i, file_name in enumerate(file_name_list):\n",
    "#         short_name = file_name[file_name.rfind('\\\\')+1:]\n",
    "#         img = Image.open(file_name)\n",
    "        \n",
    "#         if i < 0.6*1330:\n",
    "#             img.save(os.path.join(DIR_NEW_TRAIN, word_type, short_name))\n",
    "#         elif i < 0.8*1330:\n",
    "#             img.save(os.path.join(DIR_NEW_VAL, word_type, short_name))\n",
    "#         else:\n",
    "#             img.save(os.path.join(DIR_NEW_TEST, word_type, short_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8aa514c2",
   "metadata": {},
   "source": [
    "### Word Classification model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a280009a",
   "metadata": {},
   "source": [
    "Import modules and set up a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126f0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_addons\n",
      "  Using cached tensorflow_addons-0.16.1-cp38-cp38-win_amd64.whl (755 kB)\n",
      "Requirement already satisfied: typeguard>=2.7 in d:\\anaconda\\envs\\vit\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Installing collected packages: tensorflow_addons\n",
      "Successfully installed tensorflow_addons-0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66962a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import glob, random, warnings\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('TensorFlow Version ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df651c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "\n",
    "# tf.config.list_logical_devices()\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ff79b1a",
   "metadata": {},
   "source": [
    "Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e83bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = r'C:\\Users\\CAD-06\\vit\\word_train'\n",
    "VAL_PATH = r'C:\\Users\\CAD-06\\vit\\word_val'\n",
    "TEST_PATH = r'C:\\Users\\CAD-06\\vit\\word_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6075cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5751ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '1', 1: '2', 2: '3', 3: '4', 4: '가구', 5: '간섭', 6: '개폐', 7: '거실', 8: '고정', 9: '공', 10: '과다', 11: '기타', 12: '누락', 13: '단차', 14: '대피공간', 15: '도배', 16: '도장', 17: '드레스룸', 18: '들뜸', 19: '마감', 20: '마감판', 21: '면불량', 22: '문', 23: '미시공', 24: '미흡', 25: '바닥', 26: '발코니', 27: '벽', 28: '부', 29: '불량', 30: '붙박이장', 31: '석고', 32: '세부위치', 33: '세탁실', 34: '수직수평', 35: '스크레치', 36: '실링', 37: '실외기실', 38: '안', 39: '알', 40: '오염', 41: '욕실', 42: '유격', 43: '이격', 44: '이음불량', 45: '작동', 46: '잠금', 47: '전등', 48: '주방', 49: '줄눈', 50: '찍힘', 51: '창호', 52: '천장', 53: '침', 54: '코킹', 55: '콘센트', 56: '타일', 57: '태움', 58: '틀', 59: '틈새', 60: '파손', 61: '파우더', 62: '팬트리', 63: '현관', 64: '확인'}\n"
     ]
    }
   ],
   "source": [
    "words = ['1', '2', '3', '4', '가구', '간섭', '개폐', '거실', '고정', '공', '과다', '기타', '누락', '단차', '대피공간', '도배', '도장', '드레스룸', '들뜸', '마감', '마감판', '면불량', '문', '미시공', '미흡', '바닥', '발코니', '벽', '부', '불량', '붙박이장', '석고', '세부위치', '세탁실', '수직수평', '스크레치', '실링', '실외기실', '안', '알', '오염', '욕실', '유격', '이격', '이음불량', '작동', '잠금', '전등', '주방', '줄눈', '찍힘', '창호', '천장', '침', '코킹', '콘센트', '타일', '태움', '틀', '틈새', '파손', '파우더', '팬트리', '현관', '확인']\n",
    "classes = {}\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    classes[i] = word\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c17c3846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 73872 images belonging to 65 classes.\n",
      "Found 17967 images belonging to 65 classes.\n",
      "Found 1381 images belonging to 65 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 0,\n",
    "                                                          samplewise_center = True,\n",
    "                                                          samplewise_std_normalization = True)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_dataframe\n",
    "train_gen = datagen.flow_from_directory(directory = TRAIN_PATH,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = IMAGE_SIZE)\n",
    "\n",
    "val_gen = datagen.flow_from_directory(directory = VAL_PATH,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = IMAGE_SIZE)\n",
    "\n",
    "test_gen = datagen.flow_from_directory(directory = TEST_PATH,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = IMAGE_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "707196e9",
   "metadata": {},
   "source": [
    "Download pre-trained ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7096ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18440914",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet vit-keras\n",
    "\n",
    "from vit_keras import vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3696aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\vit\\lib\\site-packages\\vit_keras\\utils.py:81: UserWarning: Resizing position embeddings from 12, 12 to 7, 7\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vit_model = vit.vit_b32(\n",
    "        image_size = IMAGE_SIZE,\n",
    "        activation = 'softmax',\n",
    "        pretrained = True,\n",
    "        include_top = False,\n",
    "        pretrained_top = False,\n",
    "        classes = 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b4ce1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " embedding (Conv2D)          (None, 7, 7, 768)         2360064   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 49, 768)           0         \n",
      "                                                                 \n",
      " class_token (ClassToken)    (None, 50, 768)           768       \n",
      "                                                                 \n",
      " Transformer/posembed_input   (None, 50, 768)          38400     \n",
      " (AddPositionEmbs)                                               \n",
      "                                                                 \n",
      " Transformer/encoderblock_0   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_1   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_2   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_3   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_4   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_5   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_6   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_7   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_8   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_9   ((None, 50, 768),        7087872   \n",
      " (TransformerBlock)           (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_10  ((None, 50, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoderblock_11  ((None, 50, 768),        7087872   \n",
      "  (TransformerBlock)          (None, 12, None, None))            \n",
      "                                                                 \n",
      " Transformer/encoder_norm (L  (None, 50, 768)          1536      \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " ExtractToken (Lambda)       (None, 768)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 768)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 768)              3072      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11)                8459      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 11)               44        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 65)                780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,467,587\n",
      "Trainable params: 87,466,029\n",
      "Non-trainable params: 1,558\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "out = tf.keras.layers.Flatten()(vit_model.output)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(11, activation = tfa.activations.gelu)(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(65, 'softmax')(out)\n",
    "model = tf.keras.Model(inputs = vit_model.input, outputs = out)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b022fa19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 2.9538 - accuracy: 0.7120\n",
      "Epoch 1: val_accuracy improved from -inf to 0.98323, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14577s 3s/step - loss: 2.9538 - accuracy: 0.7120 - val_loss: 2.0485 - val_accuracy: 0.9832 - lr: 1.0000e-04\n",
      "Epoch 2/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.8904 - accuracy: 0.9710\n",
      "Epoch 2: val_accuracy improved from 0.98323 to 0.99070, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14109s 3s/step - loss: 1.8904 - accuracy: 0.9710 - val_loss: 1.5907 - val_accuracy: 0.9907 - lr: 1.0000e-04\n",
      "Epoch 3/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.6582 - accuracy: 0.9809\n",
      "Epoch 3: val_accuracy improved from 0.99070 to 0.99326, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14122s 3s/step - loss: 1.6582 - accuracy: 0.9809 - val_loss: 1.5866 - val_accuracy: 0.9933 - lr: 1.0000e-04\n",
      "Epoch 4/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.6281 - accuracy: 0.9841\n",
      "Epoch 4: val_accuracy did not improve from 0.99326\n",
      "4617/4617 [==============================] - 14125s 3s/step - loss: 1.6281 - accuracy: 0.9841 - val_loss: 1.5802 - val_accuracy: 0.9924 - lr: 1.0000e-04\n",
      "Epoch 5/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.6159 - accuracy: 0.9852\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.99326\n",
      "4617/4617 [==============================] - 14218s 3s/step - loss: 1.6159 - accuracy: 0.9852 - val_loss: 1.5772 - val_accuracy: 0.9914 - lr: 1.0000e-04\n",
      "Epoch 6/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5788 - accuracy: 0.9953\n",
      "Epoch 6: val_accuracy improved from 0.99326 to 0.99694, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14112s 3s/step - loss: 1.5788 - accuracy: 0.9953 - val_loss: 1.5562 - val_accuracy: 0.9969 - lr: 2.0000e-05\n",
      "Epoch 7/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5725 - accuracy: 0.9970\n",
      "Epoch 7: val_accuracy did not improve from 0.99694\n",
      "4617/4617 [==============================] - 14101s 3s/step - loss: 1.5725 - accuracy: 0.9970 - val_loss: 1.5565 - val_accuracy: 0.9969 - lr: 2.0000e-05\n",
      "Epoch 8/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5688 - accuracy: 0.9978\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.99694 to 0.99699, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14091s 3s/step - loss: 1.5688 - accuracy: 0.9978 - val_loss: 1.5573 - val_accuracy: 0.9970 - lr: 2.0000e-05\n",
      "Epoch 9/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5653 - accuracy: 0.9989\n",
      "Epoch 9: val_accuracy improved from 0.99699 to 0.99794, saving model to .\\model.hdf5\n",
      "4617/4617 [==============================] - 14111s 3s/step - loss: 1.5653 - accuracy: 0.9989 - val_loss: 1.5537 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 10/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5639 - accuracy: 0.9992\n",
      "Epoch 10: val_accuracy did not improve from 0.99794\n",
      "4617/4617 [==============================] - 14135s 3s/step - loss: 1.5639 - accuracy: 0.9992 - val_loss: 1.5513 - val_accuracy: 0.9979 - lr: 4.0000e-06\n",
      "Epoch 11/11\n",
      "4617/4617 [==============================] - ETA: 0s - loss: 1.5634 - accuracy: 0.9994\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.99794\n",
      "4617/4617 [==============================] - 14108s 3s/step - loss: 1.5634 - accuracy: 0.9994 - val_loss: 1.5528 - val_accuracy: 0.9978 - lr: 4.0000e-06\n"
     ]
    }
   ],
   "source": [
    "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = 1e-4)\n",
    "\n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
    "                                                 factor = 0.2,\n",
    "                                                 patience = 2,\n",
    "                                                 verbose = 1,\n",
    "                                                 min_delta = 1e-4,\n",
    "                                                 min_lr = 1e-6,\n",
    "                                                 mode = 'max')\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                 min_delta = 1e-4,\n",
    "                                                 patience = 5,\n",
    "                                                 mode = 'max',\n",
    "                                                 restore_best_weights = True,\n",
    "                                                 verbose = 1)\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n",
    "                                                  monitor = 'val_accuracy', \n",
    "                                                  verbose = 1, \n",
    "                                                  save_best_only = True,\n",
    "                                                  save_weights_only = True,\n",
    "                                                  mode = 'max')\n",
    "\n",
    "callbacks = [earlystopping, reduce_lr, checkpointer]\n",
    "\n",
    "STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n",
    "STEP_SIZE_VALID = val_gen.n // val_gen.batch_size\n",
    "\n",
    "model.fit(x = train_gen,\n",
    "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "          validation_data = val_gen,\n",
    "          validation_steps = STEP_SIZE_VALID,\n",
    "          epochs = EPOCHS,\n",
    "          shuffle = True,\n",
    "          callbacks = callbacks)\n",
    "\n",
    "model.save('word_classification_model2.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3f469c3",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940af601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\anaconda\\envs\\vit\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet vit-keras\n",
    "\n",
    "from vit_keras import vit\n",
    "from keras.models import load_model\n",
    "\n",
    "my_model = load_model('word_classification_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "304e1f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 224, 224, 3)\n",
      "{'1': 0, '2': 1, '3': 2, '4': 3, '가구': 4, '간섭': 5, '개폐': 6, '거실': 7, '고정': 8, '공': 9, '과다': 10, '기타': 11, '누락': 12, '단차': 13, '대피공간': 14, '도배': 15, '도장': 16, '드레스룸': 17, '들뜸': 18, '마감': 19, '마감판': 20, '면불량': 21, '문': 22, '미시공': 23, '미흡': 24, '바닥': 25, '발코니': 26, '벽': 27, '부': 28, '불량': 29, '붙박이장': 30, '석고': 31, '세부위치': 32, '세탁실': 33, '수직수평': 34, '스크레치': 35, '실링': 36, '실외기실': 37, '안': 38, '알': 39, '오염': 40, '욕실': 41, '유격': 42, '이격': 43, '이음불량': 44, '작동': 45, '잠금': 46, '전등': 47, '주방': 48, '줄눈': 49, '찍힘': 50, '창호': 51, '천장': 52, '침': 53, '코킹': 54, '콘센트': 55, '타일': 56, '태움': 57, '틀': 58, '틈새': 59, '파손': 60, '파우더': 61, '팬트리': 62, '현관': 63, '확인': 64} \n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.5393 - accuracy: 1.0000\n",
      "[1.5393190383911133, 1.0]\n"
     ]
    }
   ],
   "source": [
    "test_images, test_labels = next(iter(test_gen))\n",
    "labels = train_gen.class_indices\n",
    "\n",
    "print(test_images.shape)\n",
    "print(labels, '\\n')\n",
    "\n",
    "results = my_model.evaluate(test_images, test_labels, batch_size=BATCH_SIZE)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526207ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 65)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         1\n",
      "          17       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          27       1.00      1.00      1.00         1\n",
      "          28       1.00      1.00      1.00         1\n",
      "          29       1.00      1.00      1.00         2\n",
      "          33       1.00      1.00      1.00         1\n",
      "          41       1.00      1.00      1.00         1\n",
      "          48       1.00      1.00      1.00         2\n",
      "          50       1.00      1.00      1.00         1\n",
      "          51       1.00      1.00      1.00         1\n",
      "          62       1.00      1.00      1.00         1\n",
      "          64       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        16\n",
      "   macro avg       1.00      1.00      1.00        16\n",
      "weighted avg       1.00      1.00      1.00        16\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAHWCAYAAAAVVNJFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5fklEQVR4nO3dfZwU9Znv/c8Fw4iCUZNIM8iEkJ3ZhCgbQyaas8kJojGMgIi3Z+9o4onrgpP11s3mxN2zspyDGyKuOavZTVZYgmCy2QdNNoYNMgT1GAgGJZFkFVBMMhgehjA9xsdEMUDPdf/RxWw7MtM18Kua6u7v21e9mK7qrm9dXTCXv+qqanN3REREpLxhQ70BIiIilUJNU0REJCY1TRERkZjUNEVERGJS0xQREYlJTVNERCQmNU0REalYZtZoZuvN7Ckze9LM/vQozzEz+7KZdZjZVjObUrLsKjP7eTRdVTZP12mKiEilMrMGoMHdf2JmJwM/Bua4+1Mlz5kB/AkwAzgX+JK7n2tmbwa2AC2AR699n7u/0F+eRpoiIlKx3H2/u/8k+vnXwA7gjD5PuwT4uhdtBk6Nmu104EF3fz5qlA8CrQPlqWmKiEhVMLO3A+8Ffthn0RnA3pLHndG8/ub3q+64t7KM61btSOX47+0XT0ojRkSkoo2sw5Ja94nvvT747/vXHl/yKaCtZNZyd1/e93lmNhq4F/iMu78cejuOSLxpioiIHKuoQb6hSZYysxEUG+a/uPu3j/KUfUBjyePx0bx9wHl95m8YKEuHZ0VEJAwbFn4qF2lmwEpgh7t/sZ+nrQY+GZ1F+wHgJXffD9wPfNTMTjOz04CPRvP6pZGmiIiEYYkd+R3IB4H/Dmwzs8ejeX8JvA3A3ZcBaymeOdsBvApcHS173sw+DzwWvW6Ruz8/UJiapoiIVCx3/wEM/DmtF6+tvK6fZXcBd8XNU9MUEZEwYhxOrXTVX6GIiEggGmmKiEgYQ/OZZqrUNEVEJAwdnhUREZEjNNIUEZEwauDwbKZGmldOaeDWGc0suGBiojmbHt7I7JnTmdV6ISvvHPBGE5nOSCunmmpJK0e1ZDNHtcjxylTT3Lz7RZZs2lv+icehUChwy+JFLF22glWr21m3dg07OzoqLiOtnGqqJa0c1ZLNHNWSgiG4I1DaMrVFHc8d4JVDhUQztm/bSmPjBMY3NjKivp7WGTPZsP6histIK6eaakkrR7VkM0e1SAhlm6aZvcvM/iL61usvRz9X7FeKdOfzjG0Y2/t4TC5HPp+vuIy0cqqplrRyVEs2c1RLCszCTxkzYNM0s78A7qF4i6IfRZMBd5vZjQO8rs3MtpjZlicf+GbI7RURkayqgcOz5c6enQuc6e6HSmea2ReBJ4Fbj/ai0q9ySev7NOMak8vRtb+r93F3Pk8ul6u4jLRyqqmWtHJUSzZzVIuEUK6N9wDjjjK/IVpWcc48azJ79uyis3Mvhw4eZN3adqZOO7/iMtLKqaZa0spRLdnMUS0pqIHDs+VGmp8BHjKznwNHTmt9G9AEXB96Y65uGUfz6aMYXT+cm1ubaN/xLI/ufiloRl1dHfMXLOTatnn09BSYc+llNDU1V1xGWjnVVEtaOaolmzmqRUKw4jemDPAEs2HAOcAZ0ax9wGPuHus017QOz95+ccWemyQikpqRdQN/jdbxOPFD/zv47/sDP/h8poabZe8I5O49wOYUtkVERCpZBg+nhpa9U5NEREQySveeFRGRMDJ4iUho1V+hiIhIIBppiohIGDUw0lTTFBGRMIbpRCARERGJaKQpIiJh1MDh2eqvUEREJBCNNEVEJIwauLmBmqaIiIRRA4dnE2+aad0T9ob7diSeofvbiojUNo00RUQkjBo4PFv9Y2kREZFANNIUEZEwauAzzeqvUEREJBCNNEVEJIwa+ExTTVNERMLQ4VkRERE5QiNNEREJowYOz2qkKSIiEpNGmiIiEoY+00zXpoc3MnvmdGa1XsjKO5cnknHllAZundHMggsmJrL+I9KoJa2caqolrRzVks0c1ZIws/BTxmSmaRYKBW5ZvIily1awanU769auYWdHR/CczbtfZMmmvcHXWyqtWtLIqaZa0spRLdnMUS0SQmaa5vZtW2lsnMD4xkZG1NfTOmMmG9Y/FDyn47kDvHKoEHy9pdKqJY2caqolrRzVks0c1ZICGxZ+ypjMbFF3Ps/YhrG9j8fkcuTz+SHcomOXVi1p5FRTLWnlqJZs5qgWCeGYm6aZXR1yQ0REpMJppDmgz/W3wMzazGyLmW2J+wH1mFyOrv1dvY+783lyudxxbN7QSauWNHKqqZa0clRLNnNUSwpq/UQgM9vaz7QN6HcPuftyd29x95a517TF2pAzz5rMnj276Ozcy6GDB1m3tp2p084fXDUZkVYtaeRUUy1p5aiWbOaoFgmh3HWaOWA68EKf+QY8EnRD6uqYv2Ah17bNo6enwJxLL6OpqTlkBABXt4yj+fRRjK4fzs2tTbTveJZHd78UNCOtWtLIqaZa0spRLdnMUS0pyODh1NDM3ftfaLYS+Kq7/+Aoy/7V3T9eLuC1w/QfENAN9+1IPOP2iyclniEikqSRdSR2zPPES74S/Pf9ge98KlPHaAccabr73AGWlW2YIiJSQ4boM0gzuwuYBXS7+1lHWf7nwCeih3XAJOB0d3/ezHYBvwYKwGF3bxkoq/rH0iIiUu2+BrT2t9Dd/8bdz3b3s4H5wPfd/fmSp0yLlg/YMEH3nhURkVCG6DNNd99oZm+P+fQrgLuPNUsjTRERCSOBS05KL2GMpniXZBx18+wkiiPSe0tmO/CAmf04zro10hQRkcxy9+VAqDvSXwxs6nNo9kPuvs/MxgAPmtnT7r6xvxVopCkiIkFYcWQYdArscvocmnX3fdGf3cAq4JyBVqCmKSIiVc/MTgGmAt8pmTfKzE4+8jPwUWD7QOvR4VkREQkigZFh3Ny7gfOAt5pZJ3ATMALA3ZdFT7sUeMDdXyl5aQ5YFW13HfCv7r5uoCw1TRERCWOIbkPg7lfEeM7XKF6aUjrvGeA9g8nS4VkREZGYNNIUEZEghurwbJqqpmmmcV/YNO5vC7rHrYhIVlVN0xQRkaGlkaaIiEhMtdA0dSKQiIhITBppiohIEBppioiISC+NNEVEJIzqH2hqpCkiIhKXRpoiIhJELXymqaYpIiJB1ELT1OFZERGRmDLVNDc9vJHZM6czq/VCVt4Z6ou6hybnyikN3DqjmQUXTExk/UekUUs17Ze0clRLNnNUS7Iq4Euoj1tmmmahUOCWxYtYumwFq1a3s27tGnZ2dFRszubdL7Jk097g6y2VRi3Vtl/0nmUvI60c1SIhZKZpbt+2lcbGCYxvbGREfT2tM2ayYf1DFZvT8dwBXjlUCL7eUmnUUm37Re9Z9jLSylEtydNIEzCzd5nZBWY2us/81pAb0p3PM7ZhbO/jMbkc+Xw+ZESqOWlIo5Zq2y96z7KXkVaOakmBJTBlzIBN08w+DXwH+BNgu5ldUrL4liQ3TEREJGvKjTSvAd7n7nOA84D/bWZ/Gi3r9/8BzKzNzLaY2Za4H1CPyeXo2t/V+7g7nyeXy8V67WCklZOGNGqptv2i9yx7GWnlqJbk6fAsDHP33wC4+y6KjfMiM/siAzRNd1/u7i3u3jL3mrZYG3LmWZPZs2cXnZ17OXTwIOvWtjN12vmxXjsYaeWkIY1aqm2/6D3LXkZaOapFQih3c4O8mZ3t7o8DuPtvzGwWcBcwOeiG1NUxf8FCrm2bR09PgTmXXkZTU3PIiFRzrm4ZR/PpoxhdP5ybW5to3/Esj+5+KWhGGrVU237Re5a9jLRyVEvysjgyDM3cvf+FZuOBw+7edZRlH3T3TeUCXjtM/wEV5ob7dqSSc/vFk1LJEZHaM7IuudNrxvzRN4P/vu++6//NVCcecKTp7p0DLCvbMEVERKqJ7j0rIiJhZGpMmIzM3NxAREQk6zTSFBGRIGrhRCCNNEVERGLSSFNERIKohZGmmqaIiARRC01Th2dFRERi0khTRESC0EhTREREemmkKSIiYVT/QFNNczDSuidsGve41f1tRSQ0HZ4VERGRXhppiohIEBppioiISC+NNEVEJIhaGGmqaYqISBjV3zN1eFZERCQujTRFRCSIWjg8q5GmiIhITBppiohIEBppioiISK9MNc1ND29k9szpzGq9kJV3Lq/onDQyrpzSwK0zmllwwcRE1n9ENe2XtHJUSzZzVEuyzCz4lDWZaZqFQoFbFi9i6bIVrFrdzrq1a9jZ0VGROWnVsnn3iyzZtDf4ektV035JK0e1ZDNHtSRvqJqmmd1lZt1mtr2f5eeZ2Utm9ng0LSxZ1mpmPzWzDjO7sVxWZprm9m1baWycwPjGRkbU19M6YyYb1j9UkTlp1dLx3AFeOVQIvt5S1bRf0spRLdnMUS1V7WtAa5nnPOzuZ0fTIgAzGw4sAS4C3g1cYWbvHmglZZummZ1jZu+Pfn63mX3WzGbEKGJQuvN5xjaM7X08Jpcjn8+HjkklJ61a0lBN+yWtHNWSzRzVkgJLYIrB3TcCzx/DFp8DdLj7M+5+ELgHuGSgFwx49qyZ3USxA9eZ2YPAucB64EYze6+7Lz6GjRQREUnbfzGzJ4BfAn/m7k8CZwCln3F1Uuxz/So30vxvwAeBDwPXAXPc/fPAdOBj/b3IzNrMbIuZbYn7AfWYXI6u/V29j7vzeXK5XKzXDkYaOWnVkoZq2i9p5aiWbOaoluQl8ZlmaT+JprZj2LSfABPc/T3A3wP/fqw1lmuah9294O6vAjvd/WUAdz8A9PT3Indf7u4t7t4y95p49Z151mT27NlFZ+deDh08yLq17Uyddn7cOmJLIyetWtJQTfslrRzVks0c1ZK8JJpmaT+JpkGfKuzuL7v7b6Kf1wIjzOytwD6gseSp46N5/Sp3c4ODZnZS1DTfV/LGnMIATfNY1NXVMX/BQq5tm0dPT4E5l15GU1NzyIjUctKq5eqWcTSfPorR9cO5ubWJ9h3P8ujul4JmVNN+SStHtWQzR7XULjMbC+Td3c3sHIoDxueAF4FmM5tIsVleDnx8wHW5+0BBJ7j7b48y/61Ag7tvK7exrx2m/wA5qhvu25F4xu0XT0o8Q0SyZ2Rdct9F0vRn3w3++77jtovKbq+Z3Q2cB7wVyAM3ASMA3H2ZmV0PXAscBg4An3X3R6LXzgD+DhgO3FXuXJ0BR5pHa5jR/F8BvypXiIiISNLc/Yoyy+8A7uhn2Vpgbdws3XtWRESCyOIdfEJT0xQRkSBqoGdm545AIiIiWaeRpoiIBFELh2c10hQREYlJI00REQmiBgaaGmmKiIjEpZGmiIgEMWxY9Q811TRFRCQIHZ4VERGRXhppZlAa94VN4/62oHvcitQSXXIiIiIivTTSFBGRIGpgoKmmKSIiYejwrIiIiPTSSFNERILQSFNERER6aaQpIiJB1MBAU01TRETC0OFZERER6aWRpoiIBFEDA81sjTQ3PbyR2TOnM6v1Qlbeubyic6qpliunNHDrjGYWXDAxkfUfUU3vmWrJZo5qkeOVmaZZKBS4ZfEili5bwarV7axbu4adHR0VmVNNtQBs3v0iSzbtDb7eUtX0nqmWbOaoluSZWfApazLTNLdv20pj4wTGNzYyor6e1hkz2bD+oYrMqaZaADqeO8ArhwrB11uqmt4z1ZLNHNUiIQy6aZrZ15PYkO58nrENY3sfj8nlyOfzFZlTTbWkpZreM9WSzRzVkjyz8FPWDHgikJmt7jsLmGZmpwK4++yEtktERCpMFg+nhlZupDkeeBn4InB7NP265OejMrM2M9tiZlvifkA9Jpeja39X7+PufJ5cLhfrtYORRk411ZKWanrPVEs2c1SLhFCuabYAPwYWAC+5+wbggLt/392/39+L3H25u7e4e8vca9pibciZZ01mz55ddHbu5dDBg6xb287UaefHLCO+NHKqqZa0VNN7plqymaNaklfzh2fdvQf4WzP7t+jPfLnXHPOG1NUxf8FCrm2bR09PgTmXXkZTU3NF5lRTLQBXt4yj+fRRjK4fzs2tTbTveJZHd78UNKOa3jPVks0c1SIhmLvHf7LZTOCD7v6XcV/z2mHiB0hqbrhvRyo5t188KZUcEYlnZB2Jjd/O/evvB/99/8P5UzM13hzUqNHd24H2hLZFREQqWBYPp4aWmes0RUREsk73nhURkSB0yYmIiIj00khTRESCqIGBppqmiIiEocOzIiIi0ksjTRERCaIGBpoaaYqIiMSlkaaIiAShzzRFRESkl0aaNSqte8KmcY9b3d9WJBtqYaSppikiIkHUQM/U4VkREZG4NNIUEZEgauHwrEaaIiJS0czsLjPrNrPt/Sz/hJltNbNtZvaImb2nZNmuaP7jZralXJZGmiIiEsQQDjS/BtwBfL2f5b8Aprr7C2Z2EbAcOLdk+TR3/1WcIDVNEREJYqgOz7r7RjN7+wDLHyl5uBkYf6xZOjwrIiK1ZC7w3ZLHDjxgZj82s7ZyL9ZIU0REgkhioBk1stJmttzdlx/juqZRbJofKpn9IXffZ2ZjgAfN7Gl339jfOtQ0RUQks6IGeUxNspSZ/R6wArjI3Z8rWf++6M9uM1sFnAP02zR1eFZERIIYZhZ8CsHM3gZ8G/jv7v6zkvmjzOzkIz8DHwWOegZub41BtiiQTQ9vZPbM6cxqvZCVdx73/1gMaY5qGZwrpzRw64xmFlwwMZH1l6qW9yytHNWSzZy0ahkMs/BTvFy7G3gUeKeZdZrZXDP7YzP74+gpC4G3AEv7XFqSA35gZk8APwLa3X3dQFmZaZqFQoFbFi9i6bIVrFrdzrq1a9jZ0VGROapl8DbvfpElm/YGX29f1fSeqZbsZaSVk1YtlcLdr3D3Bncf4e7j3X2luy9z92XR8nnufpq7nx1NLdH8Z9z9PdF0prsvLpeVmaa5fdtWGhsnML6xkRH19bTOmMmG9Q9VZI5qGbyO5w7wyqFC8PX2VU3vmWrJXkZaOWnVMlhmFnzKmkE1TTP7kJl91sw+GnpDuvN5xjaM7X08Jpcjn8+HjkklR7VkVzW9Z6olexlp5VTbv8tKMmDTNLMflfx8DcU7LpwM3GRmNya8bSIiUkGGWfgpa8qNNEeU/NwGXOjun6N4htEn+nuRmbWZ2RYz2xL3A+oxuRxd+7t6H3fn8+RyuVivHYw0clRLdlXTe6ZaspeRVk5W/13q8CwMM7PTzOwtgLn7swDu/gpwuL8Xuftyd29x95a515S9wQIAZ541mT17dtHZuZdDBw+ybm07U6edH7eO2NLIUS3ZVU3vmWrJXkZaOdX277KSlLu5wSnAjwED3Mwa3H2/mY2O5oXbkLo65i9YyLVt8+jpKTDn0stoamoOGZFajmoZvKtbxtF8+ihG1w/n5tYm2nc8y6O7XwqeU03vmWrJXkZaOWnVMlgZHBgGZ+4++BeZnQTk3P0X5Z772mEGHyBV44b7diSecfvFkxLPEKkWI+vCDnhKzfzKj4L/vm//1DmZasXHdBs9d3+V4letiIiIAGDJ9ePMyMx1miIiIlmnG7aLiEgQWbxEJDQ1TRERCSKLl4iEpsOzIiIiMWmkKSIiQdTAQFMjTRERkbg00hQRkSBCfWl0lqlpiohIEDXQM3V4VkREJC6NNEVEJIhauORETVMSlcZ9YU97//WJZwC88NgdqeSISHapaYqISBA1MNBU0xQRkTBq4exZnQgkIiISk0aaIiISRPWPMzXSFBERiU0jTRERCaIWLjnRSFNERCQmjTRFRCQIfQm1iIhITDo8KyIiIr0y1TQ3PbyR2TOnM6v1Qlbeubyic1RLNnPG505l3fJP85N7F/Djby3guivOSySnmt4z1ZLNnLRqGQyz8FPWZKZpFgoFblm8iKXLVrBqdTvr1q5hZ0dHReaoluzmHC70cOMXv82UyxYz9ZO38amPfZh3vWNs0Ixqes9USzZz0qpF3igzTXP7tq00Nk5gfGMjI+rraZ0xkw3rH6rIHNWS3ZyuX73M4093AvCbV3/L07/oYtzppwbNqKb3TLVkMyetWgbLzIJPWTNg0zSzc83sTdHPJ5rZ58zsPjP7gpmdEnJDuvN5xjb85//xj8nlyOfzISNSy1Et2c0p9baGN3P2O8fz2PZdQddbTe+ZaslmzlD8e4ljmIWfsqbcSPMu4NXo5y8BpwBfiOZ9NcHtEknUqBPrufu2efz5bffy61deG+rNEZEKUa5pDnP3w9HPLe7+GXf/gbt/DnhHfy8yszYz22JmW+J+QD0ml6Nrf1fv4+58nlwuF+u1g5FGjmrJbg5AXd0w7r7tGr7x3S1853tPBF9/Nb1nqiWbOWn+exmMmj88C2w3s6ujn58wsxYAM/td4FB/L3L35e7e4u4tc69pi7UhZ541mT17dtHZuZdDBw+ybm07U6edH+u1g5FGjmrJbg7Asps+wU9/0cWX//l7iay/mt4z1ZLNnDT/vcjrlbu5wTzgS2b2v4BfAY+a2V5gb7Qs3IbU1TF/wUKubZtHT0+BOZdeRlNTc8iI1HJUS3Zzfv/sd/CJWeey7Wf72HzPjQDcdMdq7v/BU8Eyquk9Uy3ZzEmrlsHK3rgwPHP38k8qngw0kWKT7XT32J84v3aY8gEix+G091+fSs4Lj92RSo5IkkbWJdfb5n1je/Df9ys+dlamenGs2+i5+8tA+A9/REREKojuPSsiIkFk8Lyd4DJzcwMREZGs00hTRESCyOIlIqFppCkiIhKTRpoiIhJEDQw01TRFRCSMYTXQNXV4VkREJCY1TRERCWKovoTazO4ys24z297PcjOzL5tZh5ltNbMpJcuuMrOfR9NV5bLUNEVEpNJ9DWgdYPlFQHM0tQH/AGBmbwZuAs4FzgFuMrPTBgpS0xQRkSCG6ltO3H0j8PwAT7kE+LoXbQZONbMGYDrwoLs/7+4vAA8ycPPViUBS+dK6J+wN9+1IPOP2iyclniGSlCRGYWbWRnF0eMRyd4/3nZP/6QyKXzRyRGc0r7/5/VLTFBGRzIoa5GCbZGJ0eFZERILI8JdQ7wMaSx6Pj+b1N79fapoiIlLtVgOfjM6i/QDwkrvvB+4HPmpmp0UnAH00mtcvHZ4VEZEghg3RvQ3M7G7gPOCtZtZJ8YzYEQDuvgxYC8wAOoBXgaujZc+b2eeBx6JVLXL3gU4oUtMUEZEwhqppuvsVZZY7cF0/y+4C7oqbpcOzIiIiMWmkKSIiQeirwURERKSXRpoiIhLEUH2mmSaNNEVERGLKVNPc9PBGZs+czqzWC1l5Z3I3gEgjR7XUbs6VUxq4dUYzCy6YGHzdfWn/Zy8jrZy0ahmMofqWkzRlpmkWCgVuWbyIpctWsGp1O+vWrmFnR0dF5qiW2s7ZvPtFlmzaW/6Jx0n7P3sZaeWkVctgDTMLPmVNZprm9m1baWycwPjGRkbU19M6YyYb1j9UkTmqpbZzOp47wCuHCkHXeTTa/9nLSCsnrVrkjQZsmmb2aTNrHOg5oXTn84xtGNv7eEwuRz6fr8gc1aKcNGj/Zy8jrZys/j0elsCUNeW26fPAD83sYTP7/8zs9DQ2SkREJIvKNc1nKN71/fPA+4CnzGydmV1lZif39yIzazOzLWa2Je4H1GNyObr2d/U+7s7nyeVysV47GGnkqBblpEH7P3sZaeVk9e+xTgQq3rKvx90fcPe5wDhgKcVvtn5mgBctd/cWd2+Ze01bf097nTPPmsyePbvo7NzLoYMHWbe2nanTzo9bR2xp5KgW5aRB+z97GWnlZPXvcS2cCFTu5gav22J3P0TxK1ZWm9lJQTekro75CxZybds8enoKzLn0MpqamkNGpJajWmo75+qWcTSfPorR9cO5ubWJ9h3P8ujul4JmgPZ/FjPSykmrFnkjK978vZ+FZr/r7j87noDXDtN/gEgFueG+HYln3H7xpMQzpLaNrCOx4dvC+38e/Pf9ounNmRpuDnh49ngbpoiISDXRvWdFRCSIWrj3rJqmiIgEkcUTd0LL4rWjIiIimaSRpoiIBFEDA02NNEVEROLSSFNERIKohROBNNIUERGJSSNNEREJwpK7b0JmqGmKiEgQOjwrIiIivTTSFIkpjfvCpnF/W9A9biUZGmmKiIhIL400RUQkCKuBuxuoaYqISBA6PCsiIiK9NNIUEZEgauDorEaaIiIicWmkKSIiQdTC92mqaYqISBA6EUhERER6aaQpIiJB1MDR2WyNNDc9vJHZM6czq/VCVt65vKJzVEtt56SRceWUBm6d0cyCCyYmsv4jtF+ymZNWLfJ6mWmahUKBWxYvYumyFaxa3c66tWvY2dFRkTmqpbZz0qpl8+4XWbJpb/D1ltJ+yWZOWrUM1jAs+JQ1mWma27dtpbFxAuMbGxlRX0/rjJlsWP9QReaoltrOSauWjucO8MqhQvD1ltJ+yWZOWrXIGw3YNM2s3sw+aWYfiR5/3MzuMLPrzGxEyA3pzucZ2zC29/GYXI58Ph8yIrUc1VLbOWnVkgbtl2zmZPXvmFn4KWvKnQj01eg5J5nZVcBo4NvABcA5wFXJbp6IiFSKWrjkpFzTnOzuv2dmdcA+YJy7F8zsn4En+nuRmbUBbQB3LP0Kc69pK7shY3I5uvZ39T7uzufJ5XIxShicNHJUS23npFVLGrRfsplTTX/HKk25zzSHmVk9cDJwEnBKNP8EoN/Ds+6+3N1b3L0lTsMEOPOsyezZs4vOzr0cOniQdWvbmTrt/FivHYw0clRLbeekVUsatF+ymZPVv2PDzIJPWVNupLkSeBoYDiwA/s3MngE+ANwTdEPq6pi/YCHXts2jp6fAnEsvo6mpOWREajmqpbZz0qrl6pZxNJ8+itH1w7m5tYn2Hc/y6O6XgmZov2QzJ61a5I3M3Qd+gtk4AHf/pZmdCnwE2OPuP4oT8NphBg4QkV433LcjlZzbL56USo5kz8i65K7juPOHu4P/vr/m3AmZGm6WvSOQu/+y5OcXgW8luUEiIlKZhupwqpm1Al+ieFR0hbvf2mf53wLToocnAWPc/dRoWQHYFi3b4+6zB8rSbfRERKRimdlwYAlwIdAJPGZmq939qSPPcff/UfL8PwHeW7KKA+5+dty8zNzcQEREKtsQXad5DtDh7s+4+0GK59tcMsDzrwDuPtYa1TRFRKSSnQGU3k+yM5r3BmY2AZgIfK9k9kgz22Jmm81sTrkwHZ4VEZEgkhiFlV73H1nu7sd6h/rLgW+5e+n9Jye4+z4zewfwPTPb5u47+1uBmqaIiARhCZwIFDXIgZrkPqCx5PH4aN7RXA5c12f9+6I/nzGzDRQ/7+y3aerwrIiIVLLHgGYzmxjdjOdyYHXfJ5nZu4DTgEdL5p1mZidEP78V+CDwVN/XltJIU0REghiKC07c/bCZXQ/cT/GSk7vc/UkzWwRscfcjDfRy4B5//c0JJgFfMbMeioPIW0vPuj0aNU0REalo7r4WWNtn3sI+j//qKK97BJg8mCw1TRERCSKL94oNTZ9pioiIxKSRpkiGpHVP2NPef33iGS88dkfiGZIt1T/OVNMUEZFAauDorA7PioiIxKWRpoiIBJHEzQ2yRiNNERGRmDTSFBGRIGphFKamKSIiQejwrIiIiPTSSFNERIKo/nGmRpoiIiKxaaQpIiJB6DPNlG16eCOzZ05nVuuFrLzzWL+YOxs5qqW2c6qllvG5U1m3/NP85N4F/PhbC7juivOCZxyh/ZK9jMEalsCUNZnZpkKhwC2LF7F02QpWrW5n3do17OzoqMgc1VLbOdVUy+FCDzd+8dtMuWwxUz95G5/62Id51zvGBs0A7ZcsZsjRZaZpbt+2lcbGCYxvbGREfT2tM2ayYf1DFZmjWmo7p5pq6frVyzz+dCcAv3n1tzz9iy7GnX5q0AzQfslixrEws+BT1pRtmmb2DjP7MzP7kpl90cz+2MzeFHpDuvN5xjb85//BjsnlyOfzoWNSyVEttZ1TTbWUelvDmzn7neN5bPuu4OvWfslehhzdgE3TzD4NLANGAu8HTgAagc1mdl7SGyci2TDqxHruvm0ef37bvfz6ldeGenMkoyyBKWvKjTSvAS5y95uBjwBnuvsCoBX42/5eZGZtZrbFzLbE/YB6TC5H1/6u3sfd+Ty5XC7WawcjjRzVUts51VQLQF3dMO6+7Rq+8d0tfOd7TwRfP2i/ZDFDji7OZ5pHLks5ARgN4O57gBH9vcDdl7t7i7u3zL2mLdaGnHnWZPbs2UVn514OHTzIurXtTJ12fqzXDkYaOaqltnOqqRaAZTd9gp/+oosv//P3gq/7CO2X7GUcC7PwU9aUu05zBfCYmf0Q+K/AFwDM7HTg+aAbUlfH/AULubZtHj09BeZcehlNTc0hI1LLUS21nVNNtfz+2e/gE7POZdvP9rH5nhsBuOmO1dz/g6eC5mi/ZC/jWAzL5AHVsMzdB36C2ZnAJGC7uz892IDXDjNwgIik7rT3X594xguP3ZF4hgzeyLrkOtt92/LBf99fPDmXqU5c9o5A7v4k8GQK2yIiIhUsi4dTQ8vMdZoiIiJZp3vPiohIEFYDn2mqaYqISBA6PCsiIiK9NNIUEZEgauGSE400RUREYtJIU0REgqiFzzTVNEVEJIhaaJo6PCsiIhKTRpoiIhJELVynqZGmiIhITBppitSgNG6mfsN9OxLPALj94kmp5Eh5w6p/oKmmKSIiYejwrIiIiPTSSFNERILQJSciIiLSSyNNEREJQp9pioiISC+NNEVEJAhdciIiIhKTDs+KiIhIr0w1zU0Pb2T2zOnMar2QlXcur+gc1VLbOaplcK6c0sCtM5pZcMHERNZ/hPZLsszCT1mTmaZZKBS4ZfEili5bwarV7axbu4adHR0VmaNaajtHtQze5t0vsmTT3uDrLaX9Ur3MrNXMfmpmHWZ241GW/6GZPWtmj0fTvJJlV5nZz6PpqnJZmWma27dtpbFxAuMbGxlRX0/rjJlsWP9QReaoltrOUS2D1/HcAV45VAi+3lLaL8mzBKaymWbDgSXARcC7gSvM7N1Heeo33P3saFoRvfbNwE3AucA5wE1mdtpAeQM2TTM7xcxuNbOnzex5M3vOzHZE806NUU9s3fk8YxvG9j4ek8uRz+dDRqSWo1pqO0e1ZJP2S/KGmQWfYjgH6HD3Z9z9IHAPcEnMTZ4OPOjuz7v7C8CDQOuANZZZ4TeBF4Dz3P3N7v4WYFo075sxN0pERCQpZwClx/Y7o3l9XWZmW83sW2bWOMjX9irXNN/u7l9w964jM9y9y92/AEzo70Vm1mZmW8xsS9wPqMfkcnTt742hO58nl8vFeu1gpJGjWmo7R7Vkk/ZL8pI4PFvaT6Kp7Rg27T6K/ez3KI4m//FYayzXNHeb2f80s969YWY5M/sLXt+dX8fdl7t7i7u3zL0mXn1nnjWZPXt20dm5l0MHD7JubTtTp50f67WDkUaOaqntHNWSTdovlam0n0RT35HYPqCx5PH4aF7pOp5z999GD1cA74v72r7K3dzgY8CNwPfNbEw0Lw+sBv6gzGsHpa6ujvkLFnJt2zx6egrMufQympqaQ0aklqNaajtHtQze1S3jaD59FKPrh3NzaxPtO57l0d0vBc3QfknB0Fwi8hjQbGYTKTa8y4GPlz7BzBrcfX/0cDZw5BvS7wduKTn556PA/IHCzN2PaSvN7Gp3/2q55712mGMLEJGKdsN9O8o/KYDbL56USk61GFmXXGv74c6Xgv++P/d3Tim7vWY2A/g7YDhwl7svNrNFwBZ3X21mf02xWR4Gngeudfeno9f+EfCX0aoWl+trx9M097j728o9T01TpDapaWZTNTbNNA14eNbMtva3CBj6T51FRCQzsngHn9DKfaaZo3gdywt95hvwSCJbJCIiklHlmuYaYLS7P953gZltSGKDRESkMtXAQHPgpunucwdY9vH+lomIiFQjfZ+miIiEUQNDTTVNEREJQl9CLSIiIr000hQRkSBq4ZITjTRFRERi0khTRESCqIGBppqmiIgEUgNd85jvPRuX7j0rIklK4x631XR/2yTvPfuT3S8H/30/ZcKbMtWKNdIUEZEgdMmJiIiI9NJIU0REgqiFS07UNEVEJIga6Jk6PCsiIhKXRpoiIhJGDQw1NdIUERGJSSNNEREJQpeciIiISK9MNc1ND29k9szpzGq9kJV3Lq/oHNVS2zmqJXs5V05p4NYZzSy4YGLwdfdVTftlMMzCT1mTmaZZKBS4ZfEili5bwarV7axbu4adHR0VmaNaajtHtWQzZ/PuF1myaW/QdR5NNe2XwbIEpqzJTNPcvm0rjY0TGN/YyIj6elpnzGTD+ocqMke11HaOaslmTsdzB3jlUCHoOo+mmvaLvNExN00z+27IDenO5xnbMLb38Zhcjnw+HzIitRzVUts5qiW7OWmopv0yaDUw1Bzw7Fkzm9LfIuDs4FsjIiKSYeUuOXkM+D5H7/en9vciM2sD2gDuWPoV5l7TVnZDxuRydO3v6n3cnc+Ty+XKvm6w0shRLbWdo1qym5OGatovg6VLTmAH8Cl3n9Z3An7V34vcfbm7t7h7S5yGCXDmWZPZs2cXnZ17OXTwIOvWtjN12vnxK4kpjRzVUts5qiW7OWmopv0yWLVw9my5keZf0X9j/ZOgG1JXx/wFC7m2bR49PQXmXHoZTU3NISNSy1EttZ2jWrKZc3XLOJpPH8Xo+uHc3NpE+45neXT3S0EzoLr2i7yRuR/bF22b2dXu/tVyz3vtMMG/yVtE5Igb7tuReMbtF09KPCMtI+uSO4a645evBP99P2ncqEyNN4/nkpPPBdsKERGRClDu7Nmt/S0Chv5TZxERyY5MjQmTUe4zzRwwHXihz3wDHklki0REpCLVwtmz5ZrmGmC0uz/ed4GZbUhig0RERLJqwKbp7nMHWPbx8JsjIiKVKouXiISWmXvPioiIZJ2+hFpERIKogYGmRpoiIiJxaaQpIiJh1MBQU01TRESCqIVLTnR4VkREJCaNNEVEJIhauOTkmG/YHpdu2C4ilS6Nm8JDOjeGT/KG7R3dB4L/vm8ac2KmWrFGmiIiEkSmultC1DRFRCSMGuiaOhFIREQkJjVNEREJwhL4L1auWauZ/dTMOszsxqMs/6yZPWVmW83sITObULKsYGaPR9Pqclk6PCsiIhXLzIYDS4ALgU7gMTNb7e5PlTztP4AWd3/VzK4F/g/wsWjZAXc/O26eRpoiIhKEWfgphnOADnd/xt0PAvcAl5Q+wd3Xu/ur0cPNwPhjrVFNU0REgrAkJrM2M9tSMrX1iT0D2FvyuDOa15+5wHdLHo+M1rvZzOaUq1GHZ0VEJLPcfTmwPMS6zOxKoAWYWjJ7grvvM7N3AN8zs23uvrO/dWikKSIiYSQx1CxvH9BY8nh8NO/1m2b2EWABMNvdf3tkvrvvi/58BtgAvHegsEw1zU0Pb2T2zOnMar2QlXcG+R+LIctRLbWdo1qymZNGxpVTGrh1RjMLLpiYyPqPSGu/VIDHgGYzm2hm9cDlwOvOgjWz9wJfodgwu0vmn2ZmJ0Q/vxX4IFB6AtEbZKZpFgoFblm8iKXLVrBqdTvr1q5hZ0dHReaoltrOUS3ZzEmrls27X2TJpr3ln3gc0qplsIbikhN3PwxcD9wP7AC+6e5PmtkiM5sdPe1vgNHAv/W5tGQSsMXMngDWA7f2Oev2DTLTNLdv20pj4wTGNzYyor6e1hkz2bD+oYrMUS21naNaspmTVi0dzx3glUOF4OstlVYtlcLd17r777r777j74mjeQndfHf38EXfPufvZ0TQ7mv+Iu0929/dEf64slzVg0zSzN5nZX5vZP5nZx/ssW3rsJb5Rdz7P2IaxvY/H5HLk8/mQEanlqJbazlEt2cxJq5Y0ZLWWIbrkJFXlRppfpfhR7L3A5WZ275Hjv8AHEt0yERGpKENzHlC6yjXN33H3G93936Ph7E8onpL7loFeVHpdTdwPqMfkcnTt7+p93J3Pk8vlYr12MNLIUS21naNaspmTVi1pqKZaKk25pnmCmfU+JzpWfCewEei3cbr7cndvcfeWudf0vQ716M48azJ79uyis3Mvhw4eZN3adqZOOz/WawcjjRzVUts5qiWbOWnVkoas1lILh2fL3dzgPuB84P8emeHuXzOzLuDvg25IXR3zFyzk2rZ59PQUmHPpZTQ1NYeMSC1HtdR2jmrJZk5atVzdMo7m00cxun44N7c20b7jWR7d/VLQjLRqkTcy92P7om0zu9rdv1ruea8dJvg3eYuIpOmG+3akknP7xZMSzxhZl9xHhZ0vHAz++378afWZGm8ezyUnnwu2FSIiUvFq/vCsmW3tbxGgT51FRKSmlPtMMwdMB17oM9+ARxLZIhERqUgZHBgGV65prgFGu/vjfReY2YYkNkhERCSrBmya7j53gGUf72+ZiIjUnix+Bhmavk9TRESCiHOD9UqXmRu2i4iIZJ1GmiIiEkb1DzQ10hQREYlLI00REQmiBgaaGmmKiIjEpZGmiIgEUQuXnBzzDdvj0g3bRUTiSePG8EsunZRYa3v214eD/74//eS6TLViHZ4VERGJSYdnRUQkjEyNCZOhkaaIiEhMGmmKiEgQNTDQVNMUEZEwauHsWR2eFRERiUkjTRERCULfciIiIiK9NNIUEZEg9JmmiIiI9MpU09z08EZmz5zOrNYLWXnn8orOUS21naNasplTTbVcOaWBW2c0s+CCiYmsX44uM02zUChwy+JFLF22glWr21m3dg07OzoqMke11HaOaslmTjXVArB594ss2bQ3+HqPh1n4KWsGbJpmNtbM/sHMlpjZW8zsr8xsm5l908waQm7I9m1baWycwPjGRkbU19M6YyYb1j8UMiK1HNVS2zmqJZs51VQLQMdzB3jlUCH4emVg5UaaXwOeAvYC64EDwAzgYWBZyA3pzucZ2zC29/GYXI58Ph8yIrUc1VLbOaolmznVVEtWWQL/ZU25pplz979391uBU939C+6+193/HpjQ34vMrM3MtpjZliQ/NxAREUlTuUtOSpvq1/ssG97fi9x9ObAc4n+f5phcjq79Xb2Pu/N5crlcnJcOSho5qqW2c1RLNnOqqZasyuJnkKGVG2l+x8xGA7j7/zoy08yagJ+G3JAzz5rMnj276Ozcy6GDB1m3tp2p084PGZFajmqp7RzVks2caqolqyyBKWsGHGm6+8J+5neYWXvQDamrY/6ChVzbNo+engJzLr2MpqbmkBGp5aiW2s5RLdnMqaZaAK5uGUfz6aMYXT+cm1ubaN/xLI/ufil4jryeucc6evrGF5rtcfe3lXte3MOzIiK17ob7diSeseTSSYkN4H79257gv+9PPmFYpgacA440zWxrf4uA2jhILyIiEil3IlAOmA680Ge+AY8kskUiIlKRsniJSGjlmuYaYLS7P953gZltSGKDRESkMtXC2bPlTgSaO8Cyj4ffHBERkezSV4OJiEgQNTDQzM4N20VERLJOI00REQmjBoaaGmmKiEgQQ3XDdjNrNbOfmlmHmd14lOUnmNk3ouU/NLO3lyybH83/qZlNL5elpikiIhXLzIYDS4CLgHcDV5jZu/s8bS7wgrs3AX8LfCF67buBy4EzgVZgabS+fqlpiohIEEP0JdTnAB3u/oy7HwTuAS7p85xLgH+Mfv4WcIGZWTT/Hnf/rbv/AuiI1tcvNU0REalkZ1D8zucjOqN5R32Oux8GXgLeEvO1r5P4iUAj6wb/0bCZtUVfL5aYNDLSylEttZ2jWrKZcywZSy6dlEpOUo7l9305ZtYGtJXMWj6U9WZ1pNlW/ikVkZFWjmqp7RzVks2caqplyLj7cndvKZn6Nsx9QGPJ4/HRvKM+x8zqgFOA52K+9nWy2jRFRETieAxoNrOJZlZP8cSe1X2esxq4Kvr5vwHf8+JXfK0GLo/Orp0INAM/GihM12mKiEjFcvfDZnY9cD8wHLjL3Z80s0XAFndfDawE/snMOoDnKTZWoud9E3gKOAxc5+6FgfKy2jTTOF6d1jFx1ZK9jGrLUS3ZzKmmWjLN3dcCa/vMW1jy82vAH/Tz2sXA4rhZx/wl1CIiIrVGn2mKiIjElKmmWe5WSIEy7jKzbjPbnsT6S3IazWy9mT1lZk+a2Z8mkDHSzH5kZk9EGZ8LnVGSNdzM/sPM1iSYscvMtpnZ42a2JcGcU83sW2b2tJntMLP/Enj974xqODK9bGafCZlRkvU/on2/3czuNrORCWT8abT+J0PWcbR/i2b2ZjN70Mx+Hv15WgIZfxDV0mNmLcez/jI5fxP9HdtqZqvM7NQEMj4frf9xM3vAzMYdT4bE4O6ZmCh+gLsTeAdQDzwBvDuBnA8DU4DtCdfTAEyJfj4Z+FnoeijeHnl09PMI4IfABxKq57PAvwJrEnzPdgFvTXK/RDn/CMyLfq4HTk0wazjQBUxIYN1nAL8ATowefxP4w8AZZwHbgZMongPxf4GmQOt+w79F4P8AN0Y/3wh8IYGMScA7gQ1AS4K1fBSoi37+QkK1vKnk508Dy0L/PdP0+ilLI804t0I6bu6+keLZU4ly9/3u/pPo518DOyhzp4ljyHB3/030cEQ0Bf+Q2szGAzOBFaHXnTYzO4XiL5+VAO5+0N1fTDDyAmCnu+9OaP11wInRtWcnAb8MvP5JwA/d/VUv3knl+8D/E2LF/fxbLL3d2T8Cc0JnuPsOd//p8aw3Zs4D0XsGsJniNYChM14ueTiKBP79y+tlqWkO+nZGlSK6o/57KY4EQ697uJk9DnQDD7p78Azg74D/CfQksO5SDjxgZj+O7gKShInAs8BXo8PNK8xsVEJZUDy1/e4kVuzu+4DbgD3AfuAld38gcMx24L+a2VvM7CRgBq+/GDy0nLvvj37uAnIJZqXpj4DvJrFiM1tsZnuBTwALyz1fjk+WmmZVMrPRwL3AZ/r8X2EQ7l5w97Mp/l/sOWZ2Vsj1m9ksoNvdfxxyvf34kLtPofhtBdeZ2YcTyKijeIjrH9z9vcArFA8DBhddaD0b+LeE1n8axZHZRGAcMMrMrgyZ4e47KB5afABYBzwODHgdW8BspwpGTma2gOI1gP+SxPrdfYG7N0brvz6JDPlPWWqag76dUdaZ2QiKDfNf3P3bSWZFhxjXU/x6m5A+CMw2s10UD5mfb2b/HDgD6B054e7dwCrKfNvAMeoEOktG5N+i2ESTcBHwE3fPJ7T+jwC/cPdn3f0Q8G3g90OHuPtKd3+fu38YeIHi5/NJyZtZA0D0Z3eCWYkzsz8EZgGfiP4nIEn/AlyWcEbNy1LTjHMrpIphZkbxc7Md7v7FhDJOP3JGnpmdCFwIPB0yw93nu/t4d387xX3yPXcPOpoBMLNRZnbykZ8pnkQR/Axnd+8C9prZO6NZF1C8G0gSriChQ7ORPcAHzOyk6O/bBRQ/Ow/KzMZEf76N4ueZ/xo6o0Tp7c6uAr6TYFaizKyV4scas9391YQymkseXkLgf/9yFEN9JlLpRPHzkp9RPIt2QUIZd1P8/OcQxVHH3IRyPkTx0NJWioe0HgdmBM74PeA/ooztwMKE9895JHT2LMWzpp+IpieT2v9R1tnAluh9+3fgtAQyRlG8IfQpCe+Tz1H8Rbkd+CfghAQyHqb4PxZPABcEXO8b/i1S/Lqmh4CfUzxT980JZFwa/fxbIA/cn1AtHRTP0zjy7/+4zmztJ+PeaN9vBe4Dzkjy75sm1x2BRERE4srS4VkREZFMU9MUERGJSU1TREQkJjVNERGRmNQ0RUREYlLTFBERiUlNU0REJCY1TRERkZj+f3pd3MW1XkSQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = my_model.predict(test_images)\n",
    "print(predictions.shape)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(test_labels, axis=1) \n",
    "\n",
    "confusionmatrix = confusion_matrix(true_classes, predicted_classes)\n",
    "plt.figure(figsize = (8, 8))\n",
    "sns.heatmap(confusionmatrix, cmap = 'Blues', annot = True, cbar = True)\n",
    "\n",
    "print(classification_report(true_classes, predicted_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
